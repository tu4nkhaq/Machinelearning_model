{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1BfVYGI95eK6x8VX_ZjH2LqKoCfllstsv","authorship_tag":"ABX9TyPG2UMM++oPjabMb7OheJPu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bài tập đọc file\n","###Bài 1. Đọc file USA_Housing.csv thành các tập (X_train, y_train), (X_test, y_test) với tỉ lệ train_data: test_data là 70:30. Tập mẫu (X_) bao gồm 4 cột đầu, tập nhãn (y_) là cột cuối cùng\n","###Bài 2. Đọc file cars.csv thành các tập (X_train, y_train), (X_test, y_test) với tỉ lệ train_data: test_data là 80:20. Tập mẫu (X_) bao gồm các cột: buying, maint, doors, persons, lug_boot, safety; tập nhãn (y_) là cột: acceptability\n","###Bài 3. Đọc file cars.csv thành các tập (X_train, y_train), (X_test, y_test) với tỉ lệ train_data: test_data là 80:20. Tập mẫu (X_) bao gồm các cột: buying, doors, persons, lug_boot, safety; tập nhãn (y_) là cột: acceptability\n","###Bài 4. Đọc file StudentPerformance.csv thành các tập (X_train, y_train), (X_test, y_test) với tỉ lệ train_data: test_data là 70:30. Tập mẫu (X_) bao gồm các cột: Gioi tinh, Dan toc, Trinh do hoc van cua bo me, Dinh duong, Toan, Van; tập nhãn (y_) là cột: Anh\n","###Bài 5. Đọc file StudentPerformance.csv thành các tập (X_train, y_train), (X_test, y_test) với tỉ lệ train_data: test_data là 70:30. Tập mẫu (X_) bao gồm các cột: Dan toc, Trinh do hoc van, Dinh duong, Van, Anh; tập nhãn (y_) là cột: Toan"],"metadata":{"id":"1-AZXUWiqX96"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"5Lv9G-qke0U6","executionInfo":{"status":"error","timestamp":1700641905476,"user_tz":-420,"elapsed":495,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"5579f5aa-1688-4618-844a-8d9f058c797f"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-bab29038b9b7>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(Y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# print(X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   5355\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5359\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[0;34m(self, result, context)\u001b[0m\n\u001b[1;32m   2105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_axes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"__array_wrap__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    723\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shape of passed values is (151, 5), indices imply (151, 4)"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:4]\n","Y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:4]\n","Y_test = dt_Test[Class]\n","# print(Y_train)\n","# print(X_train)\n","X = np.insert(X_train, 0, 1, axis=1)\n","print(X)"]},{"cell_type":"code","source":[],"metadata":{"id":"NM1Iza_sq3NW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"voLXXp6JqWM1"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./cars.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.2 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:6]\n","Y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:6]\n","Y_test = dt_Test[Class]\n","print(Y_train)\n","print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPr5oW38frYN","executionInfo":{"status":"ok","timestamp":1700295485726,"user_tz":-420,"elapsed":621,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"0039025a-64d0-4d79-b931-40e38fee3bb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      unacc\n","1      unacc\n","2      unacc\n","3      unacc\n","4      unacc\n","       ...  \n","419      acc\n","420    unacc\n","421      acc\n","422      acc\n","423    unacc\n","Name: acceptability, Length: 424, dtype: object\n","     stt buying  maint  doors persons lug_boot\n","0      1  vhigh  vhigh      2       2    small\n","1      2  vhigh  vhigh      2       2    small\n","2      3  vhigh  vhigh      2       2    small\n","3      4  vhigh  vhigh      2       2      med\n","4      5  vhigh  vhigh      2       2      med\n","..   ...    ...    ...    ...     ...      ...\n","419  420  vhigh    low  5more       4      med\n","420  421  vhigh    low  5more       4      big\n","421  422  vhigh    low  5more       4      big\n","422  423  vhigh    low  5more       4      big\n","423  424  vhigh    low  5more    more    small\n","\n","[424 rows x 6 columns]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./cars.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.2 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train=dt_Train[['buying', 'doors', 'persons', 'lug_boot', 'safety']]\n","Y_train = dt_Train[Class]\n","X_test = dt_Train[['buying', 'doors', 'persons', 'lug_boot', 'safety']]\n","Y_test = dt_Test[Class]\n","print(Y_train)\n","print(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vItvDHP6hByi","executionInfo":{"status":"ok","timestamp":1700295997104,"user_tz":-420,"elapsed":440,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"986c8681-ae4d-4099-b129-d747df7f2380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      unacc\n","1      unacc\n","2      unacc\n","3      unacc\n","4      unacc\n","       ...  \n","419      acc\n","420    unacc\n","421      acc\n","422      acc\n","423    unacc\n","Name: acceptability, Length: 424, dtype: object\n","424    unacc\n","425      acc\n","426    unacc\n","427      acc\n","428      acc\n","       ...  \n","525    unacc\n","526    unacc\n","527    unacc\n","528    unacc\n","529    unacc\n","Name: acceptability, Length: 106, dtype: object\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./StudentsPerformance.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train=dt_Train[['Gioi tinh', 'Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Toan ','Van']]\n","Y_train = dt_Train[Class]\n","X_test = dt_Test[['Gioi tinh', 'Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Toan ','Van']]\n","Y_test = dt_Test[Class]\n","print(Y_train)\n","print(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kv-2V5Hxih5V","executionInfo":{"status":"ok","timestamp":1700296582840,"user_tz":-420,"elapsed":484,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"20c65925-f06f-4a3f-ef46-531dc7724472"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      74\n","1      88\n","2      93\n","3      44\n","4      75\n","       ..\n","695    86\n","696    90\n","697    70\n","698    79\n","699    59\n","Name: Anh, Length: 700, dtype: int64\n","     Gioi tinh  Dan toc  Trinh do hoc van cua bo me  Dinh duong  Toan   Van\n","700          1        5                           4           1     79   81\n","701          1        2                           2           1     57   67\n","702          2        1                           4           1     87   84\n","703          1        4                           3           1     63   64\n","704          1        2                           2           2     59   63\n","..         ...      ...                         ...         ...    ...  ...\n","995          1        5                           5           1     88   99\n","996          2        3                           2           2     62   55\n","997          1        3                           2           2     59   71\n","998          1        4                           3           1     68   78\n","999          1        4                           3           2     77   86\n","\n","[300 rows x 6 columns]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./StudentsPerformance.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train=dt_Train[['Gioi tinh', 'Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Van','Anh']]\n","Y_train = dt_Train['Toan ']\n","X_test = dt_Test[['Gioi tinh', 'Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Van','Anh']]\n","Y_test = dt_Test['Toan ']\n","print(Y_train)\n","print(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7Soa9ujjZYp","executionInfo":{"status":"ok","timestamp":1700296521878,"user_tz":-420,"elapsed":4,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"cd5023da-6f7a-4f64-ba83-681171d93e07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      72\n","1      69\n","2      90\n","3      47\n","4      76\n","       ..\n","695    79\n","696    75\n","697    59\n","698    57\n","699    66\n","Name: Toan , Length: 700, dtype: int64\n","     Gioi tinh  Dan toc  Trinh do hoc van cua bo me  Dinh duong  Van  Anh\n","700          1        5                           4           1   81   82\n","701          1        2                           2           1   67   72\n","702          2        1                           4           1   84   87\n","703          1        4                           3           1   64   67\n","704          1        2                           2           2   63   64\n","..         ...      ...                         ...         ...  ...  ...\n","995          1        5                           5           1   99   95\n","996          2        3                           2           2   55   55\n","997          1        3                           2           2   71   65\n","998          1        4                           3           1   78   77\n","999          1        4                           3           2   86   86\n","\n","[300 rows x 6 columns]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"6LGP_kemer-W"}},{"cell_type":"markdown","source":["#Bài tập Sklearn\n","###I. Sử dụng thư viện sklearn\n","Thực hiện các yêu cầu sau với các bài tập 1, 2, 3.\n","- 1) Sử dụng tập X_train, y_train để xây dựng mô hình hồi quy tuyến tính .\n","- 2) Sử dụng tập X_test, y_test để đánh giá chất lượng của mô hình trong câu a bằng các\n","độ đo NSE, R2, MAE, RMSE.\n","- Bài 1. Đọc file USA_Housing.csv thành các tập (X_train, y_train), (X_test, y_test) với tỉ lệ\n","train_data: test_data là 70:30. Tập mẫu (X_) bao gồm 4 cột đầu, tập nhãn (y_) là cột cuối\n","cùng.\n","-Bài 2. Đọc file StudentPerformance.csv thành các tập (X_train, y_train), (X_test, y_test) với\n","tỉ lệ train_data: test_data là 70:30. Tập mẫu (X_) bao gồm các cột: Gioi tinh, Dan toc, Trinh\n","do hoc van cua bo me, Dinh duong, Toan, Van; tập nhãn (y_) là cột: Anh\n","-Bài 3. Đọc file StudentPerformance.csv thành các tập (X_train, y_train), (X_test, y_test) với\n","tỉ lệ train_data: test_data là 70:30. Tập mẫu (X_) bao gồm các cột: Dan toc, Trinh do hoc van,\n","Dinh duong, Van, Anh; tập nhãn (y_) là cột: Toán.\n","###II.Code phương pháp Hồi quy tuyến tính\n","-1) Viết hàm fit để huấn luyện mô hình (tính w)\n","-2) Viết hàm predict để dự đoán nhãn của tập mẫu mới (dự đoán nhãn của tập X_tesst)\n","Có thể viết 2 hàm fit và predict trong 1 class.\n","-3) Dùng hàm fit và predict đã xây dựng để thực hiện các yêu cầu trong mục I"],"metadata":{"id":"58IBnDjhq4uD"}},{"cell_type":"markdown","source":["bài 1\n"],"metadata":{"id":"gp3H_nt-B1wt"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:4]\n","y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:4]\n","y_test = dt_Test[Class]\n","# print(y_train)\n","# print(X_train)\n","reg = LinearRegression()\n","reg.fit(X_train, y_train)\n","\n","y_pred = reg.predict(X_test)\n","y = np.array(y_test)\n","\n","print(\"Coefficient of determination: %.2f\" % r2_score (y_test, y_pred) )\n","\n","y_pred = reg.predict(X_test)\n","print('\\n')\n","\n","nse = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n","print(\"NSE: %.2f\" % nse)\n","print('\\n')\n","\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","print('\\n')\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % mae)\n","print('\\n')\n","\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE: %.2f\" % rmse)\n","print('\\n')\n","\n","print(\"Thuc te              Du doan             Chenh lech\")\n","for i in range(0, len(y)):\n","   print(\"%.2f\" % y[i],\"       \", y_pred[i], \"     \", abs(y[i] -y_pred[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzChzpaSB4f1","executionInfo":{"status":"ok","timestamp":1700899370024,"user_tz":-420,"elapsed":475,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"688f6c51-ed9e-4dce-bd39-a3b407b16831"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficient of determination: 0.66\n","\n","\n","NSE: 0.66\n","\n","\n","R2: 0.66\n","\n","\n","MAE: 141801.83\n","\n","\n","RMSE: 166096.06\n","\n","\n","Thuc te              Du doan             Chenh lech\n","1416965.98         1512127.370482949       95161.39148294902\n","1046721.98         1274000.1720516528       227278.19605165278\n","939040.00         902374.5271318164       36665.47646818368\n","1383031.33         1554429.0879855524       171397.75598555245\n","1072503.24         1192683.122158553       120179.88415855309\n","1134397.76         1322829.8125110497       188432.05451104976\n","734827.51         605150.4834136893       129677.02458631073\n","991892.32         952654.4600163945       39237.857483605505\n","1057252.58         1164091.5125457272       106838.9295457271\n","1237224.86         1115320.173402444       121904.68559755594\n","1442632.54         1315721.44060937       126911.10039063008\n","1877402.32         1711425.026696316       165977.2933036841\n","1394518.42         1475521.5635653506       81003.14856535057\n","953939.33         696184.1663420042       257755.1635579958\n","1219778.03         1488388.9186002226       268610.8846002226\n","1287325.38         1471742.8600821504       184417.47808215045\n","1739893.55         1600055.002923897       139838.55207610293\n","1213382.22         1355679.978172096       142297.75517209596\n","1007478.75         1061195.5609518806       53716.81295188062\n","1528756.13         1437436.5316017536       91319.59439824638\n","1182459.77         1314236.5957538448       131776.8237538447\n","1474546.76         1438421.7114511519       36125.05054884823\n","1078016.94         969205.9089496178       108811.03105038218\n","1453381.62         1259169.4500198686       194212.1739801315\n","1495012.97         1357206.5679763225       137806.3970236776\n","714148.41         696213.8706907155       17934.54410928453\n","894292.05         672230.5660991468       222061.4813008532\n","1159841.83         1268504.88375131       108663.05275130994\n","1195986.30         1440883.9587073172       244897.65970731713\n","1381430.63         1479603.2527643712       98172.62276437134\n","1534889.85         1381540.5188140855       153349.33418591437\n","1494125.33         1447539.7819533153       46585.548046684824\n","1356146.26         1163089.778104443       193056.48289555684\n","1454943.07         1356225.5028465446       98717.57115345541\n","774118.19         770697.3434904679       3420.8443095320836\n","889113.24         708888.3909935704       180224.84790642955\n","1211102.21         1049179.2840220802       161922.92697791965\n","658646.18         913231.236744401       254585.05204440106\n","1400104.87         1338810.1523987036       61294.71860129642\n","1262017.79         1041954.9228390702       220062.8691609297\n","553077.21         589799.5657468718       36722.353146871785\n","1203247.89         1046669.6546589904       156578.23534100945\n","1115323.01         1004653.9726707246       110669.04232927528\n","1301881.42         1415877.11737782       113995.70037782006\n","1006687.39         1152114.198394829       145426.81139482895\n","749383.07         1037739.7917323839       288356.71913238394\n","1075596.59         1215070.2100815545       139473.62308155443\n","739870.79         1051001.9347456223       311131.14114562236\n","1442945.15         1433825.599661877       9119.545338123105\n","828497.07         1080774.494340303       252277.4272403029\n","1373290.86         1370579.6244392334       2711.236560766585\n","925566.33         1029937.5341579681       104371.20285796816\n","1552536.76         1893928.9324522945       341392.16845229454\n","1335904.50         1228413.0642598215       107491.43674017838\n","960808.29         1152604.5905137951       191796.29941379512\n","1405933.02         1415710.0714875855       9777.05248758546\n","619087.69         971308.23755662       352220.54395662004\n","1329273.23         1042126.075966164       287147.1520338359\n","1261843.84         961348.6677974476       300495.1762025524\n","1175781.42         972579.6885381746       203201.72946182545\n","1437053.56         1330870.835045802       106182.72195419809\n","1594415.23         1601832.1729974262       7416.946997426217\n","931358.00         826800.377483082       104557.61791691801\n","798639.65         739752.1954604452       58887.45873955474\n","1451739.62         1347089.193033509       104650.43196649104\n","1278991.69         1188422.432909017       90569.25609098305\n"]}]},{"cell_type":"markdown","source":["bài 2"],"metadata":{"id":"rU_vySQpCFLZ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/StudentsPerformance.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train=dt_Train[['Gioi tinh', 'Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Toan ','Van']]\n","y_train = dt_Train[Class]\n","X_test = dt_Test[['Gioi tinh', 'Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Toan ','Van']]\n","y_test = dt_Test[Class]\n","# print(y_train)\n","# print(X_test)\n","reg = LinearRegression()\n","reg.fit(X_train, y_train)\n","\n","y_pred = reg.predict(X_test)\n","y = np.array(y_test)\n","\n","print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred) )\n","\n","y_pred = reg.predict(X_test)\n","print('\\n')\n","\n","nse = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n","print(\"NSE: %.2f\" % nse)\n","print('\\n')\n","\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","print('\\n')\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % mae)\n","print('\\n')\n","\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE: %.2f\" % rmse)\n","print('\\n')\n","\n","print(\"Thuc te              Du doan             Chenh lech\")\n","for i in range(0, len(y)):\n","   print(\"%.2f\" % y[i],\"       \", y_pred[i], \"     \", abs(y[i] -y_pred[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zh96NTaICG5A","executionInfo":{"status":"ok","timestamp":1700899442618,"user_tz":-420,"elapsed":1190,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"24091f9f-90d9-4631-a288-97515953a85d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficient of determination: 0.93\n","\n","\n","NSE: 0.93\n","\n","\n","R2: 0.93\n","\n","\n","MAE: 3.27\n","\n","\n","RMSE: 3.95\n","\n","\n","Thuc te              Du doan             Chenh lech\n","82.00         83.39854921476146       1.3985492147614593\n","72.00         66.35551416244873       5.644485837551272\n","87.00         81.72640281705993       5.273597182940065\n","67.00         66.42082871257257       0.5791712874274282\n","64.00         63.861782250560765       0.13821774943923515\n","65.00         66.60764025929785       1.6076402592978525\n","36.00         34.49853942658545       1.5014605734145476\n","52.00         58.04686976687342       6.046869766873421\n","79.00         83.80741690864694       4.8074169086469425\n","58.00         57.87615906706242       0.12384093293758269\n","90.00         83.00297434295199       6.997025657048013\n","85.00         85.58783652964459       0.5878365296445907\n","99.00         101.37724787160651       2.3772478716065137\n","84.00         78.8811111171535       5.118888882846505\n","74.00         69.28960517979347       4.710394820206531\n","87.00         90.16089262854872       3.160892628548723\n","72.00         72.63255472655429       0.6325547265542895\n","99.00         96.88418737988044       2.1158126201195557\n","74.00         77.80598804471613       3.8059880447161305\n","80.00         73.69917498870913       6.300825011290868\n","70.00         71.75189479431593       1.7518947943159304\n","59.00         54.80888026876647       4.1911197312335275\n","88.00         87.23652481742877       0.7634751825712272\n","42.00         45.542763343961035       3.5427633439610346\n","41.00         41.518894854521825       0.5188948545218253\n","71.00         72.96026553200578       1.9602655320057778\n","77.00         74.06176044584713       2.9382395541528723\n","57.00         52.50989077625773       4.490109223742273\n","84.00         88.67750702675863       4.677507026758633\n","37.00         40.25497432829537       3.2549743282953685\n","80.00         75.80843430552117       4.191565694478825\n","43.00         45.01482448099709       2.014824480997092\n","94.00         90.49744380213815       3.50255619786185\n","44.00         46.20224163866777       2.202241638667772\n","57.00         59.56577597063027       2.56577597063027\n","59.00         57.8136431302201       1.1863568697798996\n","84.00         78.15869994952112       5.841300050478878\n","73.00         65.00422327311347       7.9957767268865325\n","73.00         69.6979084284975       3.302091571502501\n","55.00         56.92880551419147       1.9288055141914668\n","72.00         72.34317230082046       0.3431723008204557\n","56.00         53.34695364388686       2.653046356113137\n","82.00         84.87221161032677       2.8722116103267723\n","72.00         70.85206689279707       1.147933107202931\n","47.00         52.146917877443606       5.146917877443606\n","74.00         73.22670930666776       0.7732906933322425\n","71.00         70.04966844644494       0.9503315535550598\n","68.00         62.43167536424221       5.56832463575779\n","59.00         60.49793817207092       1.497938172070917\n","86.00         81.36546570913035       4.634534290869652\n","68.00         66.20287080457854       1.797129195421462\n","65.00         68.22480131364424       3.2248013136442353\n","75.00         75.25521666964964       0.2552166696496414\n","85.00         88.2136631539874       3.2136631539873974\n","53.00         52.06159246968875       0.9384075303112525\n","92.00         93.36969417904248       1.369694179042483\n","52.00         54.663579275934005       2.663579275934005\n","72.00         66.2134768488902       5.7865231511098045\n","65.00         59.91698987233507       5.083010127664927\n","77.00         71.73984979519423       5.260150204805768\n","64.00         68.99542990525603       4.995429905256032\n","54.00         57.796181788823205       3.796181788823205\n","86.00         76.67586962844307       9.324130371556933\n","63.00         63.99209295790301       0.9920929579030116\n","59.00         59.553730971508564       0.5537309715085641\n","72.00         74.2777424472579       2.277742447257907\n","77.00         72.53251587085218       4.46748412914782\n","60.00         62.14579434160008       2.14579434160008\n","75.00         72.27928754103725       2.720712458962751\n","57.00         56.41025313560806       0.589746864391941\n","49.00         45.92663910296247       3.0733608970375315\n","74.00         71.07980705530603       2.920192944693966\n","72.00         64.99450006688868       7.005499933111324\n","79.00         76.58644598152202       2.4135540184779813\n","60.00         62.27754316774124       2.2775431677412428\n","55.00         57.70826530947177       2.70826530947177\n","70.00         72.25635805443483       2.256358054434827\n","43.00         44.619718348201275       1.6197183482012747\n","82.00         79.21083448706591       2.7891655129340904\n","82.00         83.25913046620774       1.2591304662077363\n","57.00         55.26006481341382       1.7399351865861803\n","84.00         89.53941515618483       5.53941515618483\n","82.00         84.1150844655744       2.115084465574398\n","62.00         58.9283534889085       3.071646511091501\n","79.00         80.80396382184408       1.8039638218440786\n","44.00         48.432971294684165       4.432971294684165\n","77.00         79.11665954670373       2.1166595467037297\n","32.00         36.20098433332424       4.200984333324243\n","61.00         61.841895860837866       0.8418958608378659\n","61.00         65.11247816747075       4.112478167470755\n","60.00         60.45467027046263       0.4546702704626284\n","70.00         67.01587529524234       2.9841247047576616\n","69.00         68.46864315907862       0.5313568409213758\n","77.00         81.75281537781521       4.75281537781521\n","51.00         51.68505907572504       0.685059075725043\n","73.00         66.86522617289422       6.134773827105775\n","70.00         66.67555504548767       3.3244449545123302\n","81.00         81.75569245142417       0.7556924514241672\n","54.00         59.773186882602786       5.773186882602786\n","57.00         56.02692432886039       0.9730756711396111\n","68.00         67.97213742901076       0.027862570989242386\n","73.00         75.32369590102091       2.323695901020912\n","95.00         93.40235603633911       1.597643963660886\n","87.00         86.21117905737127       0.7888209426287318\n","78.00         77.53037634454141       0.46962365545859086\n","74.00         75.93819806060968       1.938198060609679\n","75.00         73.82942581175008       1.1705741882499154\n","40.00         46.62164716410537       6.621647164105369\n","69.00         75.23433213840116       6.234332138401157\n","51.00         53.01230908102268       2.0123090810226785\n","36.00         33.22142178445077       2.7785782155492313\n","49.00         43.21345127998027       5.786548720019731\n","67.00         64.99011582571012       2.00988417428988\n","76.00         81.24746927960945       5.247469279609447\n","83.00         79.6967250083527       3.3032749916472994\n","87.00         84.10248418574066       2.897515814259336\n","64.00         58.363923808181404       5.636076191818596\n","76.00         65.26866364778394       10.731336352216061\n","68.00         70.6426709097719       2.642670909771894\n","88.00         88.93902044399942       0.939020443999425\n","92.00         89.99210686840179       2.007893131598209\n","93.00         89.45497258651686       3.5450274134831403\n","51.00         48.81921873040324       2.180781269596757\n","82.00         84.30243380008403       2.3024338000840316\n","52.00         57.57706259957323       5.577062599573232\n","58.00         62.35662844342417       4.356628443424171\n","70.00         66.36961244538266       3.630387554617343\n","76.00         69.94908347450021       6.050916525499787\n","81.00         83.31467109027493       2.3146710902749277\n","53.00         54.32155162868078       1.3215516286807798\n","57.00         62.55199358666003       5.551993586660032\n","89.00         85.42138089085286       3.578619109147141\n","58.00         58.50634772740492       0.506347727404922\n","89.00         82.28933449468683       6.710665505313173\n","45.00         49.37579942473775       4.37579942473775\n","74.00         65.05893177892564       8.94106822107436\n","57.00         63.20290079192851       6.2029007919285135\n","79.00         81.66251805727673       2.6625180572767277\n","53.00         53.39083671050846       0.39083671050845936\n","73.00         75.52220739656538       2.5222073965653777\n","46.00         51.07233175677945       5.072331756779448\n","51.00         56.33375893152169       5.333758931521693\n","36.00         40.98376397262838       4.983763972628381\n","76.00         72.90556619066302       3.0944338093369765\n","64.00         60.75464957603869       3.2453504239613125\n","84.00         84.1780680583319       0.17806805833190253\n","85.00         84.18780042902611       0.8121995709738883\n","50.00         53.553800110677926       3.5538001106779262\n","68.00         70.60911704991898       2.6091170499189786\n","69.00         72.34666453944273       3.3466645394427275\n","67.00         66.07790822675192       0.9220917732480842\n","63.00         67.97009330966796       4.970093309667959\n","93.00         87.83895532049873       5.161044679501273\n","61.00         67.65303010389056       6.653030103890558\n","55.00         60.18055657338809       5.180556573388088\n","96.00         99.1879697211928       3.1879697211928004\n","65.00         66.63002362965766       1.6300236296576571\n","81.00         78.16747126888833       2.8325287311116654\n","46.00         46.651996392974496       0.6519963929744961\n","72.00         72.4455421138889       0.4455421138889051\n","53.00         61.35709840804746       8.357098408047463\n","87.00         87.91544036011565       0.9154403601156531\n","38.00         39.39985244718371       1.3998524471837115\n","80.00         73.3982437962755       6.601756203724506\n","91.00         89.57706725520393       1.4229327447960713\n","88.00         78.89432656200056       9.105673437999442\n","52.00         51.34595838590151       0.6540416140984888\n","41.00         43.366145357682335       2.366145357682335\n","72.00         71.13978264038366       0.8602173596163425\n","51.00         46.886105867714676       4.113894132285324\n","47.00         54.94892334680293       7.948923346802928\n","76.00         73.87092803718464       2.1290719628153596\n","78.00         79.13728556934807       1.137285569348066\n","82.00         85.7775758698107       3.777575869810704\n","61.00         60.17994140837477       0.8200585916252265\n","66.00         65.11343005432832       0.8865699456716811\n","84.00         78.64165351289766       5.358346487102338\n","54.00         56.741119457837584       2.741119457837584\n","80.00         70.73058738912331       9.269412610876685\n","74.00         74.41297434755192       0.4129743475519234\n","66.00         61.39804451675884       4.601955483241163\n","70.00         73.39823463180608       3.3982346318060763\n","71.00         72.37872039619545       1.3787203961954475\n","44.00         46.344050392877975       2.344050392877975\n","54.00         53.11576279811808       0.8842372018819233\n","80.00         73.78615791014181       6.21384208985819\n","95.00         99.27053807102858       4.270538071028582\n","59.00         63.8643816506156       4.864381650615599\n","74.00         68.56457544715627       5.4354245528437275\n","48.00         46.22661840853852       1.7733815914614794\n","91.00         87.58628227139583       3.413717728604169\n","85.00         91.44629535810928       6.446295358109282\n","73.00         65.81490597088035       7.185094029119654\n","75.00         74.12509908938772       0.8749009106122827\n","69.00         63.11607738107346       5.88392261892654\n","38.00         36.3686595320471       1.6313404679529029\n","27.00         28.004432957042155       1.0044329570421553\n","79.00         75.7528337971527       3.2471662028472963\n","63.00         53.82941181085265       9.170588189147352\n","82.00         76.5987694238128       5.401230576187203\n","89.00         82.06787518360112       6.932124816398883\n","74.00         79.94590581883332       5.9459058188333245\n","41.00         46.54312633360389       5.543126333603887\n","100.00         100.4986412231804       0.4986412231803996\n","84.00         81.3586104120451       2.6413895879549045\n","77.00         76.78357840655772       0.21642159344227707\n","51.00         47.19000434847689       3.80999565152311\n","91.00         84.97074329845927       6.0292567015407315\n","72.00         75.67373935700036       3.6737393570003576\n","70.00         63.409696538887715       6.590303461112285\n","48.00         42.17925598731544       5.820744012684557\n","82.00         82.0796017898174       0.07960178981740285\n","66.00         61.82629041033434       4.173709589665663\n","66.00         61.19063360478631       4.8093663952136865\n","55.00         58.64391058506528       3.643910585065278\n","66.00         72.1578080373635       6.157808037363495\n","100.00         97.10275045240725       2.89724954759275\n","52.00         58.840091123243404       6.840091123243404\n","80.00         73.24814995463942       6.751850045360584\n","91.00         91.08044538568622       0.08044538568621817\n","67.00         66.32523314633917       0.6747668536608273\n","46.00         50.66552651117559       4.665526511175592\n","66.00         64.27947198364532       1.7205280163546774\n","65.00         63.56672413793646       1.4332758620635389\n","69.00         67.5885983918536       1.4114016081464058\n","60.00         63.455573841031395       3.455573841031395\n","52.00         54.545928732726765       2.5459287327267646\n","71.00         64.1700466435627       6.829953356437301\n","44.00         40.56816907871848       3.4318309212815166\n","51.00         56.12634801954918       5.126348019549177\n","70.00         69.08225331611109       0.9177466838889075\n","62.00         55.60928447959654       6.390715520403461\n","73.00         64.54679859639418       8.453201403605817\n","74.00         71.0480372005657       2.9519627994342983\n","90.00         85.47759656423464       4.522403435765355\n","58.00         61.982812612491784       3.9828126124917844\n","53.00         55.64107266327571       2.6410726632757076\n","57.00         60.09406071990788       3.09406071990788\n","85.00         78.92698841929717       6.0730115807028255\n","69.00         66.9935518091838       2.006448190816201\n","72.00         65.50757513579714       6.49242486420286\n","96.00         90.76038617370844       5.23961382629156\n","64.00         66.43205908636695       2.432059086366948\n","61.00         58.13657941580684       2.8634205841931575\n","61.00         67.33354450156364       6.333544501563637\n","58.00         60.88441416713832       2.884414167138317\n","80.00         78.16897843645795       1.831021563542052\n","60.00         58.43361343501439       1.5663865649856135\n","52.00         46.521685685632235       5.478314314367765\n","73.00         72.42513465011235       0.574865349887645\n","71.00         75.03725043319733       4.037250433197329\n","83.00         78.88255007196354       4.117449928036464\n","72.00         75.99521919484937       3.995219194849369\n","54.00         50.46557689683222       3.534423103167782\n","69.00         69.67347177432548       0.6734717743254777\n","62.00         57.3903192656075       4.609680734392498\n","81.00         82.90498877106212       1.904988771062122\n","100.00         100.82602447125707       0.826024471257071\n","59.00         60.664949091574684       1.6649490915746838\n","71.00         75.79638014193006       4.796380141930058\n","64.00         59.93556261116718       4.064437388832822\n","53.00         54.797103778729465       1.7971037787294648\n","100.00         101.03924941474878       1.039249414748781\n","75.00         74.92893565453878       0.07106434546122387\n","58.00         58.23366382067102       0.2336638206710191\n","72.00         70.52029940354186       1.4797005964581444\n","64.00         63.84973725143905       0.150262748560948\n","60.00         60.19027977961288       0.19027977961287945\n","67.00         75.5451368831678       8.5451368831678\n","80.00         84.19951787077298       4.199517870772979\n","100.00         99.58559787681449       0.4144021231855106\n","69.00         70.02618367913047       1.0261836791304688\n","60.00         52.794628846582754       7.205371153417246\n","61.00         63.51111446509858       2.5111144650985793\n","67.00         63.057002963021546       3.942997036978454\n","77.00         79.6084626426876       2.6084626426875985\n","60.00         58.48778415304222       1.5122158469577798\n","58.00         60.42350641626619       2.423506416266193\n","48.00         41.85009789859561       6.149902101404393\n","94.00         94.895464844354       0.8954648443539952\n","23.00         22.686522435844566       0.31347756415543415\n","78.00         75.25781690571561       2.7421830942843854\n","86.00         79.58703115918537       6.412968840814628\n","91.00         86.52973110177945       4.470268898220553\n","82.00         76.57528465649834       5.42471534350166\n","54.00         49.24460587930862       4.755394120691378\n","51.00         55.89627689967388       4.896276899673879\n","76.00         73.20321537488388       2.7967846251161177\n","45.00         46.893784954596654       1.8937849545966543\n","83.00         83.29173243920307       0.29173243920307357\n","75.00         78.72107551042484       3.721075510424839\n","78.00         79.25725790545222       1.2572579054522208\n","76.00         72.04126805558033       3.958731944419668\n","74.00         72.3557725806542       1.6442274193457962\n","62.00         59.46493165407027       2.53506834592973\n","95.00         99.21144532403784       4.2114453240378396\n","55.00         53.55469211323421       1.4453078867657894\n","65.00         69.78611090986132       4.786110909861321\n","77.00         77.83919601825536       0.8391960182553646\n","86.00         85.81846293023192       0.18153706976808337\n"]}]},{"cell_type":"markdown","source":["bài 3\n"],"metadata":{"id":"LqNU8rljx-zl"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","\n","data = pd.read_csv('./drive/MyDrive/Colab Notebooks/StudentsPerformance.csv', usecols=['Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Toan ','Van','Anh'])[['Dan toc', 'Trinh do hoc van cua bo me', 'Dinh duong', 'Van', 'Anh','Toan ']]\n","dt_Train, dt_Test = train_test_split(data, test_size=0.3, shuffle=False)\n","\n","X_train = dt_Train.iloc[:,:6]\n","y_train=dt_Train.iloc[:,5]\n","\n","X_test = dt_Test.iloc[:,:6]\n","y_test=dt_Test.iloc[:,5]\n","\n","# print('X_train: ')\n","# print( X_train)\n","# print('\\n')\n","# print('y_train: ')\n","# print( y_train)\n","# print('\\n')\n","# print('X_test: ')\n","# print( X_test)\n","# print('\\n')\n","# print('y_test: ' )\n","# print( y_test)\n","# print('\\n')\n","\n","reg = LinearRegression()\n","reg.fit(X_train, y_train)\n","\n","y_pred = reg.predict(X_test)\n","y = np.array(y_test)\n","\n","print(\"Coefficient of determination: %.2f\" % r2_score (y_test, y_pred) )\n","\n","y_pred = reg.predict(X_test)\n","print('\\n')\n","\n","nse = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n","print(\"NSE: %.2f\" % nse)\n","print('\\n')\n","\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","print('\\n')\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % mae)\n","print('\\n')\n","\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE: %.2f\" % rmse)\n","print('\\n')\n","\n","print(\"Thuc te              Du doan             Chenh lech\")\n","for i in range(0, len(y)):\n","   print(\"%.2f\" % y[i],\"       \", y_pred[i], \"     \", abs(y[i] -y_pred[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atsVsuSzsPcv","executionInfo":{"status":"ok","timestamp":1700640233250,"user_tz":-420,"elapsed":3181,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"e9359fd4-2c76-4b18-91c7-c013cf2599ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficient of determination: 1.00\n","\n","\n","NSE: 1.00\n","\n","\n","R2: 1.00\n","\n","\n","MAE: 0.00\n","\n","\n","RMSE: 0.00\n","\n","\n","Thuc te              Du doan             Chenh lech\n","79.00         78.99999999999999       1.4210854715202004e-14\n","57.00         56.99999999999999       7.105427357601002e-15\n","87.00         87.0       0.0\n","63.00         62.999999999999986       1.4210854715202004e-14\n","59.00         58.99999999999999       7.105427357601002e-15\n","62.00         61.999999999999986       1.4210854715202004e-14\n","46.00         45.99999999999998       2.1316282072803006e-14\n","66.00         65.99999999999999       1.4210854715202004e-14\n","89.00         89.00000000000001       1.4210854715202004e-14\n","42.00         41.99999999999999       7.105427357601002e-15\n","93.00         93.0       0.0\n","80.00         80.0       0.0\n","98.00         98.00000000000001       1.4210854715202004e-14\n","81.00         81.0       0.0\n","60.00         59.99999999999999       7.105427357601002e-15\n","76.00         76.00000000000001       1.4210854715202004e-14\n","73.00         73.0       0.0\n","96.00         96.00000000000001       1.4210854715202004e-14\n","76.00         76.0       0.0\n","91.00         91.00000000000001       1.4210854715202004e-14\n","62.00         61.99999999999999       7.105427357601002e-15\n","55.00         54.99999999999999       7.105427357601002e-15\n","74.00         74.0       0.0\n","50.00         49.999999999999986       1.4210854715202004e-14\n","47.00         46.99999999999998       2.1316282072803006e-14\n","81.00         81.0       0.0\n","65.00         65.0       0.0\n","68.00         67.99999999999999       1.4210854715202004e-14\n","73.00         73.0       0.0\n","53.00         52.99999999999997       2.842170943040401e-14\n","68.00         68.0       0.0\n","55.00         54.999999999999986       1.4210854715202004e-14\n","87.00         87.00000000000001       1.4210854715202004e-14\n","55.00         54.99999999999998       2.1316282072803006e-14\n","53.00         52.999999999999986       1.4210854715202004e-14\n","67.00         66.99999999999997       2.842170943040401e-14\n","92.00         92.00000000000001       1.4210854715202004e-14\n","53.00         52.999999999999986       1.4210854715202004e-14\n","81.00         81.00000000000001       1.4210854715202004e-14\n","61.00         61.0       0.0\n","80.00         79.99999999999999       1.4210854715202004e-14\n","37.00         36.999999999999986       1.4210854715202004e-14\n","81.00         81.00000000000001       1.4210854715202004e-14\n","59.00         59.0       0.0\n","55.00         54.999999999999986       1.4210854715202004e-14\n","72.00         72.0       0.0\n","69.00         68.99999999999999       1.4210854715202004e-14\n","69.00         68.99999999999999       1.4210854715202004e-14\n","50.00         49.999999999999986       1.4210854715202004e-14\n","87.00         87.00000000000001       1.4210854715202004e-14\n","71.00         70.99999999999999       1.4210854715202004e-14\n","68.00         67.99999999999999       1.4210854715202004e-14\n","79.00         78.99999999999999       1.4210854715202004e-14\n","77.00         77.0       0.0\n","58.00         58.0       0.0\n","84.00         84.00000000000001       1.4210854715202004e-14\n","55.00         54.99999999999998       2.1316282072803006e-14\n","70.00         69.99999999999999       1.4210854715202004e-14\n","52.00         51.99999999999998       2.1316282072803006e-14\n","69.00         68.99999999999999       1.4210854715202004e-14\n","53.00         52.99999999999999       7.105427357601002e-15\n","48.00         47.999999999999986       1.4210854715202004e-14\n","78.00         78.0       0.0\n","62.00         61.99999999999999       7.105427357601002e-15\n","60.00         59.99999999999999       7.105427357601002e-15\n","74.00         73.99999999999999       1.4210854715202004e-14\n","58.00         57.99999999999999       7.105427357601002e-15\n","76.00         75.99999999999999       1.4210854715202004e-14\n","68.00         67.99999999999999       1.4210854715202004e-14\n","58.00         57.99999999999999       7.105427357601002e-15\n","52.00         51.999999999999986       1.4210854715202004e-14\n","75.00         74.99999999999999       1.4210854715202004e-14\n","52.00         51.999999999999986       1.4210854715202004e-14\n","62.00         61.99999999999999       7.105427357601002e-15\n","66.00         65.99999999999999       1.4210854715202004e-14\n","49.00         48.99999999999999       7.105427357601002e-15\n","66.00         65.99999999999999       1.4210854715202004e-14\n","35.00         34.99999999999997       2.842170943040401e-14\n","72.00         71.99999999999999       1.4210854715202004e-14\n","94.00         94.00000000000001       1.4210854715202004e-14\n","46.00         45.99999999999999       7.105427357601002e-15\n","77.00         76.99999999999999       1.4210854715202004e-14\n","76.00         76.0       0.0\n","52.00         51.99999999999999       7.105427357601002e-15\n","91.00         91.0       0.0\n","32.00         31.99999999999998       2.1316282072803006e-14\n","72.00         72.0       0.0\n","19.00         18.99999999999996       3.907985046680551e-14\n","68.00         68.0       0.0\n","52.00         51.99999999999998       2.1316282072803006e-14\n","48.00         47.999999999999986       1.4210854715202004e-14\n","60.00         59.999999999999986       1.4210854715202004e-14\n","66.00         66.0       0.0\n","89.00         89.00000000000001       1.4210854715202004e-14\n","42.00         41.999999999999986       1.4210854715202004e-14\n","57.00         57.0       0.0\n","70.00         69.99999999999999       1.4210854715202004e-14\n","70.00         70.0       0.0\n","69.00         68.99999999999999       1.4210854715202004e-14\n","52.00         51.99999999999999       7.105427357601002e-15\n","67.00         66.99999999999999       1.4210854715202004e-14\n","76.00         76.0       0.0\n","87.00         87.00000000000001       1.4210854715202004e-14\n","82.00         82.00000000000001       1.4210854715202004e-14\n","73.00         72.99999999999999       1.4210854715202004e-14\n","75.00         75.0       0.0\n","64.00         63.999999999999986       1.4210854715202004e-14\n","41.00         40.999999999999986       1.4210854715202004e-14\n","90.00         90.00000000000001       1.4210854715202004e-14\n","59.00         58.999999999999986       1.4210854715202004e-14\n","51.00         50.99999999999998       2.1316282072803006e-14\n","45.00         44.999999999999986       1.4210854715202004e-14\n","54.00         53.99999999999997       2.842170943040401e-14\n","87.00         87.00000000000001       1.4210854715202004e-14\n","72.00         72.0       0.0\n","94.00         94.00000000000001       1.4210854715202004e-14\n","45.00         44.99999999999998       2.1316282072803006e-14\n","61.00         60.999999999999986       1.4210854715202004e-14\n","60.00         60.0       0.0\n","77.00         77.0       0.0\n","85.00         85.00000000000001       1.4210854715202004e-14\n","78.00         78.0       0.0\n","49.00         48.99999999999998       2.1316282072803006e-14\n","71.00         71.0       0.0\n","48.00         47.99999999999999       7.105427357601002e-15\n","62.00         62.0       0.0\n","56.00         56.0       0.0\n","65.00         64.99999999999999       1.4210854715202004e-14\n","69.00         69.0       0.0\n","68.00         67.99999999999999       1.4210854715202004e-14\n","61.00         60.99999999999999       7.105427357601002e-15\n","74.00         73.99999999999999       1.4210854715202004e-14\n","64.00         63.999999999999986       1.4210854715202004e-14\n","77.00         77.0       0.0\n","58.00         57.999999999999986       1.4210854715202004e-14\n","60.00         59.999999999999986       1.4210854715202004e-14\n","73.00         72.99999999999999       1.4210854715202004e-14\n","75.00         75.0       0.0\n","58.00         58.0       0.0\n","66.00         66.0       0.0\n","39.00         38.99999999999998       2.1316282072803006e-14\n","64.00         63.999999999999986       1.4210854715202004e-14\n","23.00         22.999999999999975       2.4868995751603507e-14\n","74.00         74.0       0.0\n","40.00         39.99999999999998       2.1316282072803006e-14\n","90.00         90.0       0.0\n","91.00         91.0       0.0\n","64.00         63.999999999999986       1.4210854715202004e-14\n","59.00         58.99999999999999       7.105427357601002e-15\n","80.00         80.0       0.0\n","71.00         70.99999999999997       2.842170943040401e-14\n","61.00         61.0       0.0\n","87.00         87.0       0.0\n","82.00         82.0       0.0\n","62.00         61.99999999999999       7.105427357601002e-15\n","97.00         97.00000000000001       1.4210854715202004e-14\n","75.00         74.99999999999999       1.4210854715202004e-14\n","65.00         64.99999999999999       1.4210854715202004e-14\n","52.00         51.999999999999986       1.4210854715202004e-14\n","87.00         87.00000000000001       1.4210854715202004e-14\n","53.00         52.99999999999999       7.105427357601002e-15\n","81.00         81.0       0.0\n","39.00         38.999999999999964       3.552713678800501e-14\n","71.00         70.99999999999999       1.4210854715202004e-14\n","97.00         97.00000000000001       1.4210854715202004e-14\n","82.00         82.0       0.0\n","59.00         58.99999999999999       7.105427357601002e-15\n","61.00         60.99999999999999       7.105427357601002e-15\n","78.00         78.0       0.0\n","49.00         48.99999999999999       7.105427357601002e-15\n","59.00         58.99999999999999       7.105427357601002e-15\n","70.00         69.99999999999999       1.4210854715202004e-14\n","82.00         82.00000000000001       1.4210854715202004e-14\n","90.00         90.00000000000001       1.4210854715202004e-14\n","43.00         42.999999999999986       1.4210854715202004e-14\n","80.00         79.99999999999999       1.4210854715202004e-14\n","81.00         81.0       0.0\n","57.00         56.99999999999999       7.105427357601002e-15\n","59.00         58.99999999999999       7.105427357601002e-15\n","64.00         64.0       0.0\n","63.00         62.99999999999998       2.1316282072803006e-14\n","71.00         70.99999999999999       1.4210854715202004e-14\n","64.00         64.0       0.0\n","55.00         54.99999999999998       2.1316282072803006e-14\n","51.00         50.999999999999986       1.4210854715202004e-14\n","62.00         62.0       0.0\n","93.00         93.00000000000001       1.4210854715202004e-14\n","54.00         53.99999999999999       7.105427357601002e-15\n","69.00         68.99999999999999       1.4210854715202004e-14\n","44.00         43.999999999999986       1.4210854715202004e-14\n","86.00         86.0       0.0\n","85.00         85.00000000000001       1.4210854715202004e-14\n","50.00         49.99999999999998       2.1316282072803006e-14\n","88.00         88.0       0.0\n","59.00         58.99999999999999       7.105427357601002e-15\n","32.00         31.99999999999997       2.842170943040401e-14\n","36.00         35.99999999999997       2.842170943040401e-14\n","63.00         63.0       0.0\n","67.00         66.99999999999999       1.4210854715202004e-14\n","65.00         65.0       0.0\n","85.00         85.0       0.0\n","73.00         72.99999999999999       1.4210854715202004e-14\n","34.00         33.99999999999998       2.1316282072803006e-14\n","93.00         93.00000000000001       1.4210854715202004e-14\n","67.00         67.0       0.0\n","88.00         88.0       0.0\n","57.00         56.99999999999999       7.105427357601002e-15\n","79.00         78.99999999999999       1.4210854715202004e-14\n","67.00         66.99999999999999       1.4210854715202004e-14\n","70.00         69.99999999999997       2.842170943040401e-14\n","50.00         49.99999999999997       2.842170943040401e-14\n","69.00         69.0       0.0\n","52.00         51.99999999999997       2.842170943040401e-14\n","47.00         46.99999999999998       2.1316282072803006e-14\n","46.00         46.0       0.0\n","68.00         67.99999999999999       1.4210854715202004e-14\n","100.00         100.0       0.0\n","44.00         43.999999999999986       1.4210854715202004e-14\n","57.00         57.0       0.0\n","91.00         91.00000000000001       1.4210854715202004e-14\n","69.00         69.0       0.0\n","35.00         34.99999999999998       2.1316282072803006e-14\n","72.00         71.99999999999999       1.4210854715202004e-14\n","54.00         53.99999999999999       7.105427357601002e-15\n","74.00         74.0       0.0\n","74.00         73.99999999999999       1.4210854715202004e-14\n","64.00         63.999999999999986       1.4210854715202004e-14\n","65.00         64.99999999999999       1.4210854715202004e-14\n","46.00         45.999999999999986       1.4210854715202004e-14\n","48.00         47.99999999999999       7.105427357601002e-15\n","67.00         66.99999999999999       1.4210854715202004e-14\n","62.00         61.99999999999999       7.105427357601002e-15\n","61.00         61.0       0.0\n","70.00         69.99999999999999       1.4210854715202004e-14\n","98.00         98.00000000000001       1.4210854715202004e-14\n","70.00         69.99999999999999       1.4210854715202004e-14\n","67.00         66.99999999999999       1.4210854715202004e-14\n","57.00         56.99999999999999       7.105427357601002e-15\n","85.00         85.0       0.0\n","77.00         76.99999999999999       1.4210854715202004e-14\n","72.00         71.99999999999999       1.4210854715202004e-14\n","78.00         77.99999999999999       1.4210854715202004e-14\n","81.00         81.0       0.0\n","61.00         61.0       0.0\n","58.00         58.0       0.0\n","54.00         53.99999999999999       7.105427357601002e-15\n","82.00         82.00000000000001       1.4210854715202004e-14\n","49.00         48.999999999999986       1.4210854715202004e-14\n","49.00         48.999999999999986       1.4210854715202004e-14\n","57.00         56.99999999999999       7.105427357601002e-15\n","94.00         94.0       0.0\n","75.00         74.99999999999999       1.4210854715202004e-14\n","74.00         74.0       0.0\n","58.00         57.99999999999999       7.105427357601002e-15\n","62.00         61.999999999999986       1.4210854715202004e-14\n","72.00         71.99999999999999       1.4210854715202004e-14\n","84.00         84.00000000000001       1.4210854715202004e-14\n","92.00         92.0       0.0\n","45.00         44.999999999999986       1.4210854715202004e-14\n","75.00         75.0       0.0\n","56.00         55.999999999999986       1.4210854715202004e-14\n","48.00         47.999999999999986       1.4210854715202004e-14\n","100.00         100.00000000000001       1.4210854715202004e-14\n","65.00         65.0       0.0\n","72.00         71.99999999999999       1.4210854715202004e-14\n","62.00         61.999999999999986       1.4210854715202004e-14\n","66.00         65.99999999999999       1.4210854715202004e-14\n","63.00         62.99999999999999       7.105427357601002e-15\n","68.00         68.0       0.0\n","75.00         74.99999999999999       1.4210854715202004e-14\n","89.00         89.00000000000001       1.4210854715202004e-14\n","78.00         77.99999999999999       1.4210854715202004e-14\n","53.00         52.999999999999986       1.4210854715202004e-14\n","49.00         48.999999999999986       1.4210854715202004e-14\n","54.00         53.99999999999998       2.1316282072803006e-14\n","64.00         63.999999999999986       1.4210854715202004e-14\n","60.00         59.99999999999999       7.105427357601002e-15\n","62.00         62.0       0.0\n","55.00         54.99999999999998       2.1316282072803006e-14\n","91.00         91.00000000000001       1.4210854715202004e-14\n","8.00         7.9999999999999645       3.552713678800501e-14\n","81.00         81.00000000000001       1.4210854715202004e-14\n","79.00         79.0       0.0\n","78.00         78.0       0.0\n","74.00         74.0       0.0\n","57.00         56.99999999999999       7.105427357601002e-15\n","40.00         39.999999999999986       1.4210854715202004e-14\n","81.00         81.0       0.0\n","44.00         43.999999999999986       1.4210854715202004e-14\n","67.00         67.0       0.0\n","86.00         86.00000000000001       1.4210854715202004e-14\n","65.00         65.0       0.0\n","55.00         54.99999999999999       7.105427357601002e-15\n","62.00         61.999999999999986       1.4210854715202004e-14\n","63.00         63.0       0.0\n","88.00         88.0       0.0\n","62.00         61.99999999999999       7.105427357601002e-15\n","59.00         58.99999999999999       7.105427357601002e-15\n","68.00         67.99999999999999       1.4210854715202004e-14\n","77.00         77.0       0.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import r2_score\n","from sklearn.linear_model import LinearRegression   #import hàm LinearRegression(để tính toán) của gói linear_model trong thư viện sklearn\n","from sklearn.model_selection import train_test_split   #import hàm train_test_split(để phân tách dữ liệu) từ gói model_selection của thư viện sklearn\n","import numpy as np\n","\n","#đọc vào 1 file\n","data = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","\n","# data : dữ liệu bị phân tác là\n","# test_size=0.3 : số xác định kích thước của tập kiểm tra\n","# dữ liệu kiểm tra: dt_Test = 30% của tổng số dữ liệu ban đầu \"data\"\n","# shuffle=False: dữ liệu đc lấy theo thứ tự từ trên xuống không xáo trộn(shuffle = True: xáo trộn r mới lấy)\n","# dữ liệu để học: dt_Train = phần còn lại\n","dt_Train, dt_Test = train_test_split(data,test_size=0.3, shuffle=False)\n","\n","X_train = dt_Train.iloc[:,:5]   #lấy tất cả các dòng và các cột từ 0->4 {cột: 0,1,2,3,4}\n","y_train = dt_Train.iloc[:,5]    #lấy tất cả các dòng và cột số 5\n","X_test = dt_Test.iloc[:,:5]\n","y_test = dt_Test.iloc[:,5]\n","\n","\n","print(\"\\nDữ liệu Train: \\n\", dt_Train)\n","print(\"\\n\\nDữ liệu Test: \\n\", dt_Test)\n","print(\"\\n\\nCác đặc trưng Train: \\n\", X_train)\n","print(\"\\n\\nCác nhãn Train: \\n\", y_train)\n","\n","\n","#tiến hành học dữ liệu bằng phương thức fix\n","reg = LinearRegression().fit(X_train,y_train)# thực hiện tính toán trả về model(trả về 1 đối tượng có đầy đủ kết quả)\n","\n","#reg.score(X_test,y_test)    #hệ số xác định của dự đoán\n","\n","print(\"w = \",reg.coef_)     # trả về vecter trọng số w\n","print(\"w0 = \",reg.intercept_) #số hạng độc lập (b)\n","\n","y_pred = reg.predict(X_test)    #giá trị dự đoán với các mẫu dữ liệu X_test\n","y = np.array(y_test)\n","#print(\"\\nHệ số xác định: %.2f\" % r2_score(y_test,y_pred))\n","print(\"\\nThuc te\\t\\t\\tDu doan\\t\\t\\tchenh lech\\n\")\n","for i in range(0,len(y)):\n","    print(\"%.2f\" %y[i], \"\\t\", y_pred[i], \"\\t\", abs(y[i]-y_pred[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3rTO2q1bp8-","executionInfo":{"status":"ok","timestamp":1700882253390,"user_tz":-420,"elapsed":475,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"8d062e6d-bc41-4593-ea1b-e119a01ec249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Dữ liệu Train: \n","      TB_ThuNhapKhuVuc  TB_tuoinha  TB_dientich  TB_sophong  Dansokhuvuc  \\\n","0         79545.45857    5.682861     7.009188        4.09  23086.80050   \n","1         79248.64245    6.002900     6.730821        3.09  40173.07217   \n","2         61287.06718    5.865890     8.512727        5.13  36882.15940   \n","3         63345.24005    7.188236     5.586729        3.26  34310.24283   \n","4         59982.19723    5.040555     7.839388        4.23  26354.10947   \n","..                ...         ...          ...         ...          ...   \n","146       69277.89125    6.142592     7.811724        5.15  43717.91309   \n","147       67946.71468    5.403176     5.671136        3.21  35197.94257   \n","148       90975.11327    4.740683     8.320352        6.45  44426.21365   \n","149       61885.54644    5.405266     6.914196        3.22  48726.95130   \n","150       69401.84722    7.042355     8.097044        3.44  29293.07405   \n","\n","              Gia  \n","0    1.059034e+06  \n","1    1.505891e+06  \n","2    1.058988e+06  \n","3    1.260617e+06  \n","4    6.309435e+05  \n","..            ...  \n","146  1.415074e+06  \n","147  1.043484e+06  \n","148  1.544380e+06  \n","149  1.352136e+06  \n","150  1.505727e+06  \n","\n","[151 rows x 6 columns]\n","\n","\n","Dữ liệu Test: \n","      TB_ThuNhapKhuVuc  TB_tuoinha  TB_dientich  TB_sophong  Dansokhuvuc  \\\n","151       72969.12127    6.188576     8.076944        4.20  31755.72136   \n","152       75582.45976    5.506473     6.774986        4.33  31946.30387   \n","153       56241.22677    5.623087     6.818671        2.18  32745.36812   \n","154       79412.83547    6.087360     7.481321        3.26  17568.73376   \n","155       79618.23685    4.566166     6.669888        3.49  29636.50711   \n","..                ...         ...          ...         ...          ...   \n","212       79471.23882    5.867270     8.197289        6.21  36616.16883   \n","213       55879.50379    6.407946     5.370387        2.07  37360.06356   \n","214       40503.54133    6.881779     6.566175        3.14  45979.47754   \n","215       83394.40783    5.601487     5.905024        2.08  30487.52462   \n","216       64501.22645    6.001642     7.293477        5.33  43869.30998   \n","\n","              Gia  \n","151  1.416966e+06  \n","152  1.046722e+06  \n","153  9.390400e+05  \n","154  1.383031e+06  \n","155  1.072503e+06  \n","..            ...  \n","212  1.594415e+06  \n","213  9.313580e+05  \n","214  7.986397e+05  \n","215  1.451740e+06  \n","216  1.278992e+06  \n","\n","[66 rows x 6 columns]\n","\n","\n","Các đặc trưng Train: \n","      TB_ThuNhapKhuVuc  TB_tuoinha  TB_dientich  TB_sophong  Dansokhuvuc\n","0         79545.45857    5.682861     7.009188        4.09  23086.80050\n","1         79248.64245    6.002900     6.730821        3.09  40173.07217\n","2         61287.06718    5.865890     8.512727        5.13  36882.15940\n","3         63345.24005    7.188236     5.586729        3.26  34310.24283\n","4         59982.19723    5.040555     7.839388        4.23  26354.10947\n","..                ...         ...          ...         ...          ...\n","146       69277.89125    6.142592     7.811724        5.15  43717.91309\n","147       67946.71468    5.403176     5.671136        3.21  35197.94257\n","148       90975.11327    4.740683     8.320352        6.45  44426.21365\n","149       61885.54644    5.405266     6.914196        3.22  48726.95130\n","150       69401.84722    7.042355     8.097044        3.44  29293.07405\n","\n","[151 rows x 5 columns]\n","\n","\n","Các nhãn Train: \n"," 0      1.059034e+06\n","1      1.505891e+06\n","2      1.058988e+06\n","3      1.260617e+06\n","4      6.309435e+05\n","           ...     \n","146    1.415074e+06\n","147    1.043484e+06\n","148    1.544380e+06\n","149    1.352136e+06\n","150    1.505727e+06\n","Name: Gia, Length: 151, dtype: float64\n","w =  [ 2.10579544e+01  1.75821607e+05  1.31432841e+05 -7.76910535e+03\n","  1.50053581e+01]\n","w0 =  -2689248.149321477\n","\n","Thuc te\t\t\tDu doan\t\t\tchenh lech\n","\n","1416965.98 \t 1440868.9820302734 \t 23903.003030273365\n","1046721.98 \t 1206702.0654817978 \t 159980.08948179777\n","939040.00 \t 854353.8301249887 \t 84686.17347501137\n","1383031.33 \t 1274902.3130591535 \t 108129.01894084644\n","1072503.24 \t 1084414.3208270585 \t 11911.08282705862\n","1134397.76 \t 1152790.5623767157 \t 18392.804376715794\n","734827.51 \t 652039.1341858758 \t 82788.37381412426\n","991892.32 \t 909779.0449605254 \t 82113.27253947465\n","1057252.58 \t 1117577.1643527057 \t 60324.5813527056\n","1237224.86 \t 1211122.8552046884 \t 26102.003795311553\n","1442632.54 \t 1415503.0577106331 \t 27129.48328936682\n","1877402.32 \t 1826365.4349746862 \t 51036.88502531382\n","1394518.42 \t 1457763.349522321 \t 63244.93452232098\n","953939.33 \t 735723.7069587465 \t 218215.62294125347\n","1219778.03 \t 1277441.8392755426 \t 57663.80527554266\n","1287325.38 \t 1397866.9035363514 \t 110541.52153635141\n","1739893.55 \t 1656589.9479237087 \t 83303.60707629123\n","1213382.22 \t 1304455.3193756556 \t 91073.09637565562\n","1007478.75 \t 1077072.433902016 \t 69593.68590201589\n","1528756.13 \t 1497730.9520258326 \t 31025.173974167323\n","1182459.77 \t 1226379.9385949266 \t 43920.1665949265\n","1474546.76 \t 1438862.3393655587 \t 35684.422634441406\n","1078016.94 \t 1154944.5385084022 \t 76927.59850840224\n","1453381.62 \t 1446195.8880024026 \t 7185.735997597454\n","1495012.97 \t 1404667.5687848553 \t 90345.39621514478\n","714148.41 \t 819947.5689332108 \t 105799.15413321077\n","894292.05 \t 793568.7950541805 \t 100723.25234581949\n","1159841.83 \t 1206124.0535102487 \t 46282.222510248655\n","1195986.30 \t 1442464.429026361 \t 246478.13002636097\n","1381430.63 \t 1334647.7120431429 \t 46782.91795685701\n","1534889.85 \t 1696591.157520664 \t 161701.30452066404\n","1494125.33 \t 1500874.8007102422 \t 6749.470710242167\n","1356146.26 \t 1213889.2573259631 \t 142257.00367403682\n","1454943.07 \t 1401639.2810519058 \t 53303.7929480942\n","774118.19 \t 878032.8846962438 \t 103914.69689624384\n","889113.24 \t 774756.5478475499 \t 114356.6910524501\n","1211102.21 \t 1224736.5600603162 \t 13634.349060316337\n","658646.18 \t 862930.0121356645 \t 204283.82743566448\n","1400104.87 \t 1347838.9737007422 \t 52265.897299257806\n","1262017.79 \t 1155429.2131104623 \t 106588.5788895376\n","553077.21 \t 631394.5682707625 \t 78317.35567076248\n","1203247.89 \t 1202314.297649548 \t 933.5923504519742\n","1115323.01 \t 935872.3650644575 \t 179450.64993554237\n","1301881.42 \t 1256600.5695072557 \t 45280.847492744215\n","1006687.39 \t 1036058.2980501219 \t 29370.911050121882\n","749383.07 \t 772090.6090511917 \t 22707.536451191758\n","1075596.59 \t 1086109.8910302883 \t 10513.304030288244\n","739870.79 \t 876141.6080618161 \t 136270.8144618161\n","1442945.15 \t 1572252.027175787 \t 129306.88217578689\n","828497.07 \t 998837.9078814173 \t 170340.84078141733\n","1373290.86 \t 1307483.1661349284 \t 65807.69486507168\n","925566.33 \t 855689.9701416749 \t 69876.3611583251\n","1552536.76 \t 1633370.7586271642 \t 80833.99462716421\n","1335904.50 \t 1340166.2044227924 \t 4261.703422792489\n","960808.29 \t 1053738.2649560142 \t 92929.9738560142\n","1405933.02 \t 1400808.1962430933 \t 5124.8227569067385\n","619087.69 \t 739593.471429442 \t 120505.77782944206\n","1329273.23 \t 1323466.137063573 \t 5807.090936426772\n","1261843.84 \t 1065480.38886136 \t 196363.45513864\n","1175781.42 \t 1129690.3103755373 \t 46091.107624462806\n","1437053.56 \t 1319682.5805705935 \t 117370.97642940655\n","1594415.23 \t 1594431.994172562 \t 16.768172561889514\n","931358.00 \t 864479.5568704959 \t 66878.43852950411\n","798639.65 \t 902193.5633899164 \t 103553.90918991645\n","1451739.62 \t 1269160.486395494 \t 182579.13860450592\n","1278991.69 \t 1299701.7685686448 \t 20710.079568644753\n"]}]},{"cell_type":"markdown","source":["xây dựng hàm predict"],"metadata":{"id":"fOWJr0W0q9mx"}},{"cell_type":"code","source":["import numpy as np\n","\n","class LinearRegression:\n","    def __init__(self, learning_rate=0.01, n_iterations=1000):\n","        self.learning_rate = learning_rate\n","        self.n_iterations = n_iterations\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        # Thêm cột 1 vào X để tính bias\n","        X = np.insert(X, 0, 1, axis=1)\n","\n","        # Khởi tạo trọng số ngẫu nhiên\n","        self.weights = np.random.rand(X.shape[1])\n","\n","        for _ in range(self.n_iterations):\n","            # Dự đoán giá trị\n","            predictions = self.predict(X)\n","\n","            # Tính độ lỗi\n","            errors = predictions - y\n","\n","            # Cập nhật trọng số\n","            gradient = np.dot(X.T, errors) / len(y)\n","            self.weights -= self.learning_rate * gradient\n","\n","    def predict(self, X):\n","        # Thêm cột 1 vào X để tính bias\n","        X = np.insert(X, 0, 1, axis=1)\n","\n","        # Dự đoán giá trị\n","        predictions = np.dot(X, self.weights)\n","\n","        return predictions\n","\n","# Ví dụ về cách sử dụng\n","# Tạo mô hình\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:4]\n","y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:4]\n","y_test = dt_Test[Class]\n","model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n","\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","# In kết quả\n","print(\"Weights:\", model.weights)\n","print(\"Predictions:\", y_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"3Ca2XM6Xq8vs","executionInfo":{"status":"error","timestamp":1700882229726,"user_tz":-420,"elapsed":340,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"71a8f01a-e4d6-4a9d-a223-0480662ae5c5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-b12fa878d1bb>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-b12fa878d1bb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Thêm cột 1 vào X để tính bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Khởi tạo trọng số ngẫu nhiên\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   5355\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5359\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[0;34m(self, result, context)\u001b[0m\n\u001b[1;32m   2105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_axes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"__array_wrap__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    723\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shape of passed values is (151, 5), indices imply (151, 4)"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","class MultipleLinearRegression:\n","    def __init__(self):\n","        self.coefficients = None\n","\n","    def fit(self, X, y):\n","        # Add a column of ones for the intercept term\n","        X_extended = np.c_[np.ones(X.shape[0]), X]\n","\n","        # β=(XTX)−1XTY\n","        self.coefficients = np.linalg.inv(X_extended.T @ X_extended) @ X_extended.T @ y\n","\n","    def predict(self, X):\n","        # Add a column of ones for the intercept term\n","        X_extended = np.c_[np.ones(X.shape[0]), X]\n","\n","        # Calculate the predicted values\n","        y_pred = X_extended @ self.coefficients\n","        return y_pred\n","\n","    def evaluate(self, X, y):\n","        y_pred = self.predict(X)\n","        mse = np.mean((y - y_pred)**2)\n","        r2 = 1 - mse / np.var(y)\n","        return mse, r2\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:4]\n","y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:4]\n","y_test = dt_Test[Class]\n","\n","model = MultipleLinearRegression()\n","\n","# Fit the model to the training data\n","model.fit(X_train, y_train)\n","\n","# Evaluate the model on the testing data\n","mse, r2 = model.evaluate(X_test, y_test)\n","\n","y_pred = model.predict(X_test)\n","\n","# Print the results\n","print('Mean Squared Error:', mse)\n","print('R-squared:', r2)\n","print(\"Predictions:\", y_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJ3bRbbXdq7u","executionInfo":{"status":"ok","timestamp":1700884383268,"user_tz":-420,"elapsed":307,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"4f039cbc-fbda-45a7-f0b6-f768d50cf386"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 27587902061.52803\n","R-squared: 0.6596882036906659\n","Predictions: [1512127.37048296 1274000.17205165  902374.5271318  1554429.08798556\n"," 1192683.12215855 1322829.81251105  605150.48341366  952654.46001638\n"," 1164091.51254573 1115320.17340243 1315721.44060937 1711425.02669633\n"," 1475521.56356536  696184.16634197 1488388.91860022 1471742.86008217\n"," 1600055.00292389 1355679.97817209 1061195.56095185 1437436.53160175\n"," 1314236.59575383 1438421.71145115  969205.9089496  1259169.45001986\n"," 1357206.56797632  696213.8706907   672230.56609912 1268504.8837513\n"," 1440883.95870732 1479603.25276439 1381540.51881407 1447539.78195331\n"," 1163089.77810444 1356225.50284654  770697.34349044  708888.39099355\n"," 1049179.28402207  913231.23674439 1338810.15239871 1041954.92283905\n","  589799.56574684 1046669.65465898 1004653.97267072 1415877.11737782\n"," 1152114.19839481 1037739.79173238 1215070.21008155 1051001.93474561\n"," 1433825.59966187 1080774.49434028 1370579.62443924 1029937.53415796\n"," 1893928.93245231 1228413.06425981 1152604.59051378 1415710.07148759\n","  971308.2375566  1042126.07596614  961348.66779743  972579.68853816\n"," 1330870.8350458  1601832.17299745  826800.37748305  739752.19546041\n"," 1347089.1930335  1188422.43290902]\n"]}]},{"cell_type":"markdown","source":["Code thuần linear"],"metadata":{"id":"wVjiHm4IzBUX"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","\n","class LinearRegression:\n","    def __init__(self):\n","        self.coefficients = None\n","\n","    def fit(self, X, y):\n","        # Add a column of ones for the intercept term\n","        X_extended = np.c_[np.ones(X.shape[0]), X]\n","\n","        # β=(XTX)−1XTY\n","        self.coefficients = np.linalg.inv(X_extended.T @ X_extended) @ X_extended.T @ y\n","\n","    def predict(self, X):\n","        # Add a column of ones for the intercept term\n","        X_extended = np.c_[np.ones(X.shape[0]), X]\n","\n","        # Calculate the predicted values\n","        y_pred = X_extended@self.coefficients\n","        return y_pred\n","\n","# r2\n","def r2(Y_pred, Y_test):\n","    return r2_score(Y_test, Y_pred)\n","# nse\n","def nse(Y_pred, Y_test):\n","    sse = np.sum((Y_test - Y_pred) ** 2)\n","\n","    y_mean = np.mean(Y_test)\n","    sst = np.sum((Y_test - y_mean) ** 2)\n","\n","    return 1 - (sse / sst)\n","# mae\n","def mae(Y_pred, Y_test):\n","    return np.mean(np.abs(Y_test - Y_pred))\n","\n","# rmse\n","def rmse(Y_pred, Y_test):\n","    return np.sqrt(np.mean((Y_test - Y_pred) ** 2))\n","\n","def error(y,y_pred):\n","    sum=0\n","    for i in range(0,len(y)):\n","        sum = sum + abs(y[i] - y_pred[i])\n","    return sum/len(y)\n","\n","df_data = pd.read_csv('./drive/MyDrive/Colab Notebooks/cars.csv')\n","dt_train, dt_test = train_test_split(df_data, train_size=0.7, shuffle=False)\n","\n","X_tr = dt_train.iloc[:,:10]\n","Y_tr = dt_train.iloc[:,10]\n","X_te = dt_test.iloc[:,:10]\n","Y_te = dt_test.iloc[:,10]\n","\n","\n","reg = LinearRegression()\n","reg.fit(X_tr,Y_tr)\n","\n","Y_pred = reg.predict(X_te)\n","\n","y = np.array(Y_te)\n","print(\"Coefficient of determination: %.5f\" % r2(Y_te,Y_pred))\n","\n","print(\"Thuc te     Du doan         \\tChenh lech\")\n","for i in range(0,len(y)):\n","    print(\"%.2f\" % y[i], \"   \", Y_pred[i], \"   \", abs(y[i]-Y_pred[i]))\n","\n","print(\"NSE = \",nse(Y_pred,Y_te))\n","print(\"MAE = \",mae(Y_pred,Y_te))\n","print(\"RMSE = \",rmse(Y_pred,Y_te))\n","# print('Tính theo công thức, Hồi quy tuyến tính có bias:')\n","# print ('w', reg.coefficients)\n","# print('Giá trị dự đoán mẫu mới: ', Y_pred)\n","# print('Giá trị dự đoán tập huấn luyện: ', reg.predict(X_tr))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGY0KvHpeIl8","executionInfo":{"status":"ok","timestamp":1701343142944,"user_tz":-420,"elapsed":436,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"d0dca709-f726-4b5c-b6b0-5f93e981b215"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficient of determination: 0.83286\n","Thuc te     Du doan         \tChenh lech\n","23875.00     21152.347779245272     2722.6522207547278\n","8916.50     5494.0430436976785     3422.4569563023215\n","45400.00     39258.336968978896     6141.663031021104\n","6229.00     5601.912475844638     627.0875241553622\n","11845.00     8148.9472628502435     3696.0527371497565\n","7788.00     7321.08630184877     466.91369815123016\n","8558.00     8444.049487551305     113.95051244869501\n","16503.00     19774.40820996048     3271.4082099604784\n","18920.00     20186.12654681249     1266.1265468124911\n","17075.00     18545.363230178107     1470.3632301781072\n","28176.00     24254.867213906742     3921.132786093258\n","19699.00     24445.844239039856     4746.844239039856\n","10198.00     10346.773955353066     148.7739553530664\n","16558.00     21614.71556703776     5056.7155670377615\n","36000.00     43015.80845204362     7015.808452043617\n","8948.00     11305.360744419895     2357.360744419895\n","6488.00     5226.820916592026     1261.1790834079738\n","11694.00     11227.398226620873     466.6017733791268\n","11850.00     14059.052433191242     2209.052433191242\n","30760.00     25946.152588311164     4813.847411688836\n","13499.00     21399.313766371884     7900.313766371884\n","8845.00     11964.064108282593     3119.064108282593\n","10595.00     11964.064108282593     1369.0641082825932\n","16900.00     18003.445745058696     1103.445745058696\n","31400.50     34667.6266509083     3267.1266509083\n","35550.00     33081.2871129892     2468.712887010799\n","9495.00     7920.109949025496     1574.8900509745044\n","34184.00     31749.875700933262     2434.124299066738\n","7689.00     8364.540017186644     675.540017186644\n","16430.00     11059.996929206724     5370.0030707932765\n","7099.00     6367.943313080938     731.0566869190616\n","17450.00     15782.759165283853     1667.2408347161472\n","7395.00     5711.893430996424     1683.1065690035757\n","13295.00     15725.55484692722     2430.55484692722\n","11248.00     11473.03437616725     225.03437616725023\n","6692.00     5653.516780351481     1038.4832196485186\n","7463.00     9075.364861667007     1612.364861667007\n","16515.00     17516.943703229277     1001.9437032292772\n","5151.00     -1284.9316430599265     6435.9316430599265\n","9295.00     13631.46391407998     4336.463914079981\n","32528.00     23492.29442270102     9035.70557729898\n","8058.00     6642.265834025339     1415.7341659746608\n","15510.00     13929.023883571757     1580.9761164282427\n","16630.00     15636.172349823757     993.827650176243\n","9895.00     13233.946895823927     3338.9468958239268\n","37028.00     23595.503031714707     13432.496968285293\n","6692.00     5445.412626623682     1246.5873733763183\n","8013.00     9647.643613428543     1634.6436134285432\n","12940.00     17043.306188073664     4103.306188073664\n","10295.00     8258.883749267408     2036.1162507325917\n","10245.00     11913.625744827517     1668.6257448275173\n","6295.00     5776.538856632851     518.4611433671489\n","8499.00     10694.587339817328     2195.587339817328\n","15580.00     15564.430621128908     15.56937887109234\n","12170.00     13781.247920665784     1611.2479206657845\n","5389.00     6592.4050744767155     1203.4050744767155\n","9639.00     14596.915517648093     4957.915517648093\n","13495.00     11611.056580828785     1883.9434191712153\n","8495.00     10231.093870809007     1736.093870809007\n","11549.00     15014.441254112557     3465.4412541125566\n","6918.00     6257.08913252242     660.9108674775798\n","6229.00     5601.912475844638     627.0875241553622\n","NSE =  0.850791870483481\n","MAE =  2661.3401020474316\n","RMSE =  3592.737724875135\n"]}]},{"cell_type":"markdown","source":["code thuần không sử dụng bias"],"metadata":{"id":"i8JKFExCzIeJ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","\n","class LinearRegression:\n","    def __init__(self):\n","        self.coefficients = None\n","\n","    def fit(self, X, y):\n","        # β=(XTX)−1XTY\n","        self.coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n","\n","    def predict(self, X):\n","        # Calculate the predicted values\n","        y_pred = np.dot(X,self.coefficients)\n","        return y_pred\n","\n","# r2\n","def r2(Y_pred, Y_test):\n","    return r2_score(Y_test, Y_pred)\n","# nse\n","def nse(Y_pred, Y_test):\n","    sse = np.sum((Y_test - Y_pred) ** 2)\n","\n","    y_mean = np.mean(Y_test)\n","    sst = np.sum((Y_test - y_mean) ** 2)\n","\n","    return 1 - (sse / sst)\n","# mae\n","def mae(Y_pred, Y_test):\n","    return np.mean(np.abs(Y_test - Y_pred))\n","\n","# rmse\n","def rmse(Y_pred, Y_test):\n","    return np.sqrt(np.mean((Y_test - Y_pred) ** 2))\n","\n","def error(y,y_pred):\n","    sum=0\n","    for i in range(0,len(y)):\n","        sum = sum + abs(y[i] - y_pred[i])\n","    return sum/len(y)\n","\n","df_data = pd.read_csv('./drive/MyDrive/Colab Notebooks/cars.csv')\n","dt_train, dt_test = train_test_split(df_data, train_size=0.7, shuffle=False)\n","\n","X_tr = dt_train.iloc[:,:10]\n","Y_tr = dt_train.iloc[:,10]\n","X_te = dt_test.iloc[:,:10]\n","Y_te = dt_test.iloc[:,10]\n","\n","print(X_tr)\n","reg = LinearRegression()\n","reg.fit(X_tr,Y_tr)\n","\n","print(reg.coefficients)\n","Y_pred = reg.predict(X_te)\n","\n","y = np.array(Y_te)\n","print(\"Coefficient of determination: %.5f\" % r2(Y_te,Y_pred))\n","\n","# print(\"Thuc te     Du doan         \\tChenh lech\")\n","# for i in range(0,len(y)):\n","#     print(\"%.2f\" % y[i], \"   \", Y_pred[i], \"   \", abs(y[i]-Y_pred[i]))\n","\n","print(\"NSE = \",nse(Y_pred,Y_te))\n","print(\"MAE = \",mae(Y_pred,Y_te))\n","print(\"RMSE = \",rmse(Y_pred,Y_te))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xta0NuSCzNjL","executionInfo":{"status":"ok","timestamp":1701352478580,"user_tz":-420,"elapsed":334,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"c9e9c9a4-9f1b-4db1-b4ad-d3640eecdaf9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["     Rui_ro  Nguyen_Lieu  So_cua  chieu_dai  Chieu_rong  Chieu_cao  Can_nang  \\\n","0        -1            1       4      188.8        68.9       55.5      2952   \n","1         2            1       4      176.6        66.2       54.3      2337   \n","2         0            1       4      198.9        68.4       56.7      3285   \n","3         3            1       2      168.9        68.3       50.2      2778   \n","4         0            1       4      177.8        66.5       55.5      2443   \n","..      ...          ...     ...        ...         ...        ...       ...   \n","138       0            1       4      169.7        63.6       59.1      3110   \n","139       1            1       2      169.1        66.0       51.0      2293   \n","140       0            1       4      175.4        65.2       54.1      2465   \n","141       0            1       4      177.8        66.5       55.5      2410   \n","142       1            1       4      166.8        64.2       54.1      1945   \n","\n","     Kich_thuoc  Ma_luc  Vong_tua  \n","0           141     114      5400  \n","1           109     102      5500  \n","2           120      95      5000  \n","3           151     143      5500  \n","4           122      64      4650  \n","..          ...     ...       ...  \n","138          92      62      4800  \n","139         110     100      5500  \n","140         110     101      5800  \n","141         122      84      4800  \n","142          91      68      5000  \n","\n","[143 rows x 10 columns]\n","0   -258.093171\n","1    155.749973\n","2    -82.603214\n","3      2.214845\n","4   -164.658669\n","5   -172.258849\n","6      5.746229\n","7     89.471878\n","8     28.575112\n","9      0.826501\n","dtype: float64\n","Coefficient of determination: 0.79901\n","NSE =  0.8395426449089942\n","MAE =  2611.55637888703\n","RMSE =  3725.7103287986447\n"]}]},{"cell_type":"markdown","source":["Dùng thư viện"],"metadata":{"id":"nI3P_Sirz6Tc"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","# r2\n","def r2(Y_pred, Y_test):\n","    return r2_score(Y_test, Y_pred)\n","# nse\n","def nse(Y_pred, Y_test):\n","    sse = np.sum((Y_test - Y_pred) ** 2)\n","\n","    y_mean = np.mean(Y_test)\n","    sst = np.sum((Y_test - y_mean) ** 2)\n","\n","    return 1 - (sse / sst)\n","# mae\n","def mae(Y_pred, Y_test):\n","    return np.mean(np.abs(Y_test - Y_pred))\n","\n","# rmse\n","def rmse(Y_pred, Y_test):\n","    return np.sqrt(np.mean((Y_test - Y_pred) ** 2))\n","\n","def error(y,y_pred):\n","    sum=0\n","    for i in range(0,len(y)):\n","        sum = sum + abs(y[i] - y_pred[i])\n","    return sum/len(y)\n","\n","df_data = pd.read_csv('./drive/MyDrive/Colab Notebooks/cars.csv')\n","dt_train, dt_test = train_test_split(df_data, train_size=0.7, shuffle=False)\n","\n","X_tr = dt_train.iloc[:,:10]\n","Y_tr = dt_train.iloc[:,10]\n","X_te = dt_test.iloc[:,:10]\n","Y_te = dt_test.iloc[:,10]\n","\n","reg = LinearRegression().fit(X_tr,Y_tr)\n","\n","\n","Y_pred = reg.predict(X_te)\n","y = np.array(Y_te)\n","print(\"Coefficient of determination: %.5f\" % r2(Y_te,Y_pred))\n","\n","print(\"Thuc te     Du doan         \\tChenh lech\")\n","for i in range(0,len(y)):\n","    print(\"%.2f\" % y[i], \"   \", Y_pred[i], \"   \", abs(y[i]-Y_pred[i]))\n","\n","print(\"NSE = \",nse(Y_pred,Y_te))\n","print(\"MAE = \",mae(Y_pred,Y_te))\n","print(\"RMSE = \",rmse(Y_pred,Y_te))\n","\n","# w0=reg.intercept_\n","# w1=reg.coef_\n","# print('Tính w bằng thư viện scikit-learn:')\n","# print(' w0 = ', w0, 'w_1 = ', w1)\n","# print('y_test_hat: ', Y_pred)\n","# print('y_train_hat: ', reg.predict(X_tr))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmyuGYGKz-2K","executionInfo":{"status":"ok","timestamp":1701343126391,"user_tz":-420,"elapsed":378,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"9acb5b2e-393f-45f6-ee6c-a4307297a11d"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficient of determination: 0.83286\n","Thuc te     Du doan         \tChenh lech\n","23875.00     21152.34777924644     2722.65222075356\n","8916.50     5494.043043703059     3422.456956296941\n","45400.00     39258.336968972566     6141.663031027434\n","6229.00     5601.912475853605     627.0875241463946\n","11845.00     8148.94726285369     3696.0527371463104\n","7788.00     7321.086301854681     466.91369814531936\n","8558.00     8444.049487550059     113.95051244994102\n","16503.00     19774.40820995433     3271.4082099543302\n","18920.00     20186.12654681383     1266.12654681383\n","17075.00     18545.363230181814     1470.3632301818143\n","28176.00     24254.867213898484     3921.132786101516\n","19699.00     24445.844239042388     4746.844239042388\n","10198.00     10346.773955358352     148.7739553583524\n","16558.00     21614.715567040228     5056.715567040228\n","36000.00     43015.80845203959     7015.808452039593\n","8948.00     11305.360744419522     2357.3607444195222\n","6488.00     5226.820916599623     1261.1790834003768\n","11694.00     11227.39822662562     466.60177337437926\n","11850.00     14059.052433192803     2209.0524331928027\n","30760.00     25946.152588316458     4813.847411683542\n","13499.00     21399.313766377352     7900.313766377352\n","8845.00     11964.064108278471     3119.0641082784714\n","10595.00     11964.064108278471     1369.0641082784714\n","16900.00     18003.44574506166     1103.445745061661\n","31400.50     34667.626650898426     3267.1266508984263\n","35550.00     33081.28711299645     2468.712887003552\n","9495.00     7920.109949034202     1574.8900509657979\n","34184.00     31749.875700931894     2434.1242990681058\n","7689.00     8364.540017185136     675.5400171851361\n","16430.00     11059.99692920725     5370.003070792751\n","7099.00     6367.943313078387     731.0566869216127\n","17450.00     15782.75916529425     1667.2408347057499\n","7395.00     5711.893431003569     1683.1065689964307\n","13295.00     15725.554846930594     2430.554846930594\n","11248.00     11473.034376167867     225.03437616786687\n","6692.00     5653.516780360529     1038.483219639471\n","7463.00     9075.364861671798     1612.3648616717983\n","16515.00     17516.943703230194     1001.9437032301939\n","5151.00     -1284.931643057862     6435.931643057862\n","9295.00     13631.463914083899     4336.463914083899\n","32528.00     23492.294422705323     9035.705577294677\n","8058.00     6642.265834023929     1415.7341659760714\n","15510.00     13929.023883581438     1580.976116418562\n","16630.00     15636.17234982617     993.8276501738292\n","9895.00     13233.946895824338     3338.946895824338\n","37028.00     23595.50303171917     13432.49696828083\n","6692.00     5445.41262663329     1246.5873733667104\n","8013.00     9647.643613432898     1634.6436134328978\n","12940.00     17043.306188071932     4103.306188071932\n","10295.00     8258.883749276385     2036.116250723615\n","10245.00     11913.625744830351     1668.6257448303513\n","6295.00     5776.53885663033     518.46114336967\n","8499.00     10694.587339821985     2195.5873398219846\n","15580.00     15564.430621131352     15.569378868647618\n","12170.00     13781.247920675232     1611.2479206752323\n","5389.00     6592.405074476788     1203.4050744767883\n","9639.00     14596.91551764909     4957.915517649089\n","13495.00     11611.056580835851     1883.9434191641485\n","8495.00     10231.0938708178     1736.0938708178\n","11549.00     15014.441254114237     3465.4412541142374\n","6918.00     6257.0891325267     660.9108674732997\n","6229.00     5601.912475853605     627.0875241463946\n","NSE =  0.8507918704835489\n","MAE =  2661.340102046047\n","RMSE =  3592.7377248743173\n"]}]},{"cell_type":"markdown","source":["cách"],"metadata":{"id":"7z3XXkaHeJX2"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import math\n","class GDLinearRegression:\n","  def __init__(self, lr, step):\n","    '''\n","    Khởi tạo learning rate và số lượng step update weigh\n","    '''\n","    self.lr = lr\n","    self.step = step\n","\n","  def fit(self, X, y):\n","    '''\n","    Train the model với đầu vào là tập train data X cùng nhãn Y\n","    X là ma trận MxN trong đó M là số lượng điểm dữ liệu, mỗi điểm dữ liệu có N chiều.\n","    Trường hợp dữ liệu 1 chiều thì X là vector cột Mx1\n","    '''\n","    # lấy ra số lượng điểm train_size và số chiều dữ liệu n_features\n","    self.n_features = X.shape[1] if len(X.shape) > 1 else 1\n","    train_size = len(X) # số lượng sample\n","\n","    # chuẩn hóa lại định dạng dữ liệu\n","    X = X.values.reshape(-1, self.n_features)\n","    y = y.values.reshape([-1, 1])\n","\n","    # ta muốn thực hiện dự đoán y = X.T*W + bias, ta đưa bias vào W (W|bias) và 1 cột toàn 1 vào X (X|one),\n","    # lúc đó việc tính toán thuận tiện hơn y = (X|one).T*(W|bias) => đây là trick để tính toán cho nhanh\n","    one = np.ones([train_size, 1])\n","    X = np.concatenate([X, one], 1)\n","\n","    # tạo weight chính là parameters ta sẽ optimize trong quá trình train\n","    self.weight = np.zeros([self.n_features + 1, 1])\n","    print('x shape: ', X.shape, '- y shape: ', y.shape, '- weight shape: ', self.weight.shape, 'train_size: ', train_size)\n","\n","    # mảng lưu lại toàn bộ giá trị loss trong quá trình train\n","    self.train_loss = []\n","\n","    # train\n","    for i in range(self.step):\n","\n","      loss = np.sum((y - np.dot(X, self.weight)) ** 2)\n","      delta = np.dot(X.T, ( np.dot(X, self.weight) - y))\n","\n","      # update weight\n","      self.weight = self.weight - (self.lr/train_size) * delta\n","\n","      # tính trung bình loss\n","      loss = loss/train_size\n","      self.train_loss.append(loss.item())\n","\n","  def predict(self, X):\n","    '''\n","    Thực hiện dự đoán\n","    '''\n","    # chuẩn hóa format dữ liệu như ta đã làm trong lúc train\n","    X = X.values.reshape(-1, self.n_features)\n","\n","    # thêm cột one như ta đã làm trong lúc train\n","    one = np.ones([len(X), 1])\n","    X = np.concatenate([X, one], 1)\n","\n","    # thực hiện dự đoán, đơn giản là nhân ma trận\n","    y_hat = np.dot(X, self.weight)\n","\n","    return y_hat\n","\n","  def print_weight(self):\n","    '''\n","    In weigt đã học được\n","    '''\n","    print(self.weight)\n","\n","  def get_train_loss(self):\n","    '''\n","    Trả về train loss đã lưu trong quá trình train\n","    '''\n","    return self.train_loss\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:4]\n","y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:4]\n","y_test = dt_Test[Class]\n","model = GDLinearRegression(0.1,100)\n","\n","# Fit the model to the training data\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","# Print the results\n","print(\"Predictions:\", y_pred)\n","# X_1 = np.random.rand(1, 500)\n","# print(X_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpNGUAVWuoqH","executionInfo":{"status":"ok","timestamp":1700889934085,"user_tz":-420,"elapsed":304,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"36c36d2f-2bd4-4503-ee4d-fa39cfb120bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x shape:  (151, 5) - y shape:  (151, 1) - weight shape:  (5, 1) train_size:  151\n","Predictions: [[nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]\n"," [nan]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n","  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n","<ipython-input-21-c79e3d96dce7>:42: RuntimeWarning: overflow encountered in square\n","  loss = np.sum((y - np.dot(X, self.weight)) ** 2)\n","<ipython-input-21-c79e3d96dce7>:46: RuntimeWarning: invalid value encountered in subtract\n","  self.weight = self.weight - (self.lr/train_size) * delta\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","class LinearRegression:\n","    def __init__(self, print_cost=False):\n","        self.learning_rate = 0.01\n","        self.total_iterations = 1000\n","        self.print_cost = print_cost\n","\n","    def y_hat(self, X, w):\n","        return np.dot(w.T, X)\n","\n","    def cost(self, yhat, y):\n","      # n = len(X)\n","      # sum_error = 0\n","      # for i in range(n):\n","      #   sum_error += (y[i] - (weight*X*X[i] + bias))**2\n","        C = 1 / self.m * np.sum(np.power(yhat - y, 2))\n","\n","        return C\n","\n","    def gradient_descent(self, w, X, y, yhat):\n","        dCdW = 2 / self.m * np.dot(X, (yhat - y).T)\n","        w -= self.learning_rate * dCdW\n","\n","        return w\n","\n","    def main(self, X, y):\n","        # Add x1 = 1\n","        ones = np.ones((1, X.shape[1]))\n","        X = np.append(ones, X, axis=0)\n","\n","        self.m = X.shape[1]\n","        self.n = X.shape[0]\n","\n","        w = np.zeros((self.n, 1))\n","\n","        for it in range(self.total_iterations + 1):\n","            yhat = self.y_hat(X, w)\n","            cost = self.cost(yhat, y)\n","\n","            if it % 2000 == 0 and self.print_cost:\n","                print(f\"Cost at iteration {it} is {cost}\")\n","\n","            w = self.gradient_descent(w, X, y, yhat)\n","\n","        return w\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./drive/MyDrive/Colab Notebooks/USA_Housing.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","Class = df.keys()[-1]\n","\n","X_train = dt_Train.iloc[:,:4]\n","y_train = dt_Train[Class]\n","X_test = dt_Test.iloc[:,:4]\n","y_test = dt_Test[Class]\n","model = GDLinearRegression(0.1,100)\n","regression = LinearRegression()\n","w = regression.main(X_test, y_test)\n","print(w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"Eug-G65090dm","executionInfo":{"status":"error","timestamp":1700890431332,"user_tz":-420,"elapsed":453,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"a97b6603-a1d2-4399-b657-41e526e18021"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-1f3c9aed2c73>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGDLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mregression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-1f3c9aed2c73>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_iterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-1f3c9aed2c73>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(self, yhat, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     ):\n\u001b[0;32m-> 2113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marraylike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# for binary ops, use our custom dunder methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_dispatch_ufunc_to_dunder_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/ops_dispatch.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__mul__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_method_SERIES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cmp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TEST_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/roperator.py\u001b[0m in \u001b[0;36mrsub\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,4) (66,) "]}]}]}