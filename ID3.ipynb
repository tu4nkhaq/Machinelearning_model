{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1q7MrrQh6Eah3CI5x_4tIRsnJD5NPZpA1","authorship_tag":"ABX9TyP6AcCt1ZU/k80iMHTHzioD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"IESgFr6mPoJW"}},{"cell_type":"markdown","source":["bai1"],"metadata":{"id":"1UdfbQAfPs8K"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8uv3pM-FPa_u","executionInfo":{"status":"ok","timestamp":1703053739685,"user_tz":-420,"elapsed":3,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"b11320d2-a3db-45fa-d785-f93458b5bb7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.8780487804878049\n","Precision:  0.9029239766081871\n","Recall:  0.8650793650793651\n","F1:  0.8788055296675986\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn import preprocessing\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/winedata.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.2 , shuffle = True)\n","\n","le=preprocessing.LabelEncoder()\n","df=df.apply(le.fit_transform)\n","\n","dt_Train, dt_Test = train_test_split(df, test_size=0.2 , shuffle = True)\n","\n","X_train = dt_Train.drop(['Class'],axis=1)\n","y_train = dt_Train['Class']\n","X_test = dt_Test.drop(['Class'],axis=1)\n","y_test = dt_Test['Class']\n","\n","pla = DecisionTreeClassifier(criterion='entropy')\n","pla.fit(X_train, y_train)\n","y_predict = pla.predict(X_test)\n","y_predict=np.array(y_predict)\n","y_test=np.array(y_test)\n","\n","print(\"Accuracy: \", accuracy_score(y_test, y_predict))\n","print(\"Precision: \", precision_score(y_test, y_predict,average=\"macro\"))\n","print(\"Recall: \", recall_score(y_test, y_predict,average=\"macro\"))\n","print(\"F1: \", f1_score(y_test, y_predict,average=\"macro\"))"]},{"cell_type":"markdown","source":["bai2"],"metadata":{"id":"G6jKYnjfQVji"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/cars_chu.csv\",usecols=['buying','maint','doors','persons','lug_boot','safety','acceptability'])\n","\n","# Split the data into training and testing sets\n","dt_Train, dt_Test = train_test_split(data, test_size=0.3, shuffle=True)\n","\n","X_train = dt_Train.drop(['acceptability'],axis=1)\n","y_train = dt_Train['acceptability']\n","X_test = dt_Test.drop(['acceptability'],axis=1)\n","y_test = dt_Test['acceptability']\n","encoders = {}\n","for col in X_train.columns:\n","    encoders[col]=LabelEncoder()\n","    X_train[col] = encoders[col].fit_transform(X_train[col])\n","\n","pla = DecisionTreeClassifier(criterion='entropy')\n","pla.fit(X_train, y_train)\n","for col in X_test.columns:\n","    X_test[col] = encoders[col].transform(X_test[col])\n","\n","y_predict = pla.predict(X_test)\n","y_predict=np.array(y_predict)\n","y_test=np.array(y_test)\n","# Evaluation\n","print(\"Accuracy: \", accuracy_score(y_test, y_predict))\n","print(\"Precision: \", precision_score(y_test, y_predict,average=\"macro\"))\n","print(\"Recall: \", recall_score(y_test, y_predict,average=\"macro\"))\n","print(\"F1: \", f1_score(y_test, y_predict,average=\"macro\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-whQxcqvQb04","executionInfo":{"status":"ok","timestamp":1703053777511,"user_tz":-420,"elapsed":1091,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"5c9b81cd-d46f-47ea-f0c2-fc4575c3d0ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  1.0\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n"]}]},{"cell_type":"markdown","source":["bai3"],"metadata":{"id":"DDfjRzNEQvOu"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/bankmarketing.csv\",usecols=['age', 'marital', 'education', 'housing', 'duration','bankrupt'])\n","\n","# Split the data into training and testing sets\n","dt_Train, dt_Test = train_test_split(data, test_size=0.2, shuffle=True)\n","\n","X_train = dt_Train.drop(['bankrupt'],axis=1)\n","y_train = dt_Train['bankrupt']\n","X_test = dt_Test.drop(['bankrupt'],axis=1)\n","y_test = dt_Test['bankrupt']\n","label=['marital','education','housing']\n","\n","encoders = {}\n","for i in label:\n","    encoders[i]=LabelEncoder()\n","    X_train[i] = encoders[i].fit_transform(X_train[i])\n","\n","pla = DecisionTreeClassifier(criterion='entropy')\n","pla.fit(X_train, y_train)\n","for i in label:\n","    X_test[i] = encoders[i].transform(X_test[i])\n","\n","y_predict = pla.predict(X_test)\n","y_predict=np.array(y_predict)\n","y_test=np.array(y_test)\n","# Evaluation\n","print(\"Accuracy: \", accuracy_score(y_test, y_predict))\n","print(\"Precision: \", precision_score(y_test, y_predict,average=\"macro\"))\n","print(\"Recall: \", recall_score(y_test, y_predict,average=\"macro\"))\n","print(\"F1: \", f1_score(y_test, y_predict,average=\"macro\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVChMXT7QwhF","executionInfo":{"status":"ok","timestamp":1702456475192,"user_tz":-420,"elapsed":5,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"a504f3be-7711-443a-a841-8dcea6c13839"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.4074074074074074\n","Precision:  0.4041666666666667\n","Recall:  0.4052197802197802\n","F1:  0.4041379310344827\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import pprint\n","from sklearn import metrics\n","import random\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","class ID3:\n","    def __init__(self):\n","        self.Class = None\n","        self.Y_label = []\n","        self.dt_Train=None\n","        self.tree=None\n","    def find_entropy(self,df):\n","      values = df[self.Class].unique()\n","      entropy = 0\n","      for value in values:\n","        prob = df[self.Class].value_counts()[value]/len(df[self.Class])\n","        entropy += -prob * np.log2(prob)\n","      return float(entropy)\n","\n","    def find_entropy_attribute(self,df,attribute):\n","      target_values = df[self.Class].unique()\n","      attribute_values = df[attribute].unique()\n","      avg_entropy = 0\n","      for value in attribute_values:\n","        entropy = 0\n","        for value1 in target_values:\n","          num = len(df[attribute][df[attribute] == value][df[self.Class] == value1]) #len(df[(df[attribute] == value) & (df[Class] == value1)])\n","          den = len(df[attribute][df[attribute] == value])\n","          prob = num/den\n","          entropy += -prob * np.log2(prob + 0.000001)\n","        avg_entropy += (den/len(df))*entropy\n","      return float(avg_entropy)\n","    #Find Winner: Gain\n","    def find_winner(self,df):\n","      IG = []\n","      for key in df.keys()[:-1]:\n","        IG.append(self.find_entropy(df) - self.find_entropy_attribute(df, key))\n","      return df.keys()[:-1][np.argmax(IG)]\n","      #np.argmax(IG): trả về chỉ mục có Gain lớn nhất\n","      #df.keys()[:-1][np.argmax(IG)] truy cập vào danh sách tên cột, loại bỏ cột cuối cùng và trả về tên cột có chỉ mục tương ứng với Information Gain (IG) lớn nhất.\n","    #lấy bảng phụ\n","    def get_subtable(df, attribute, value):\n","      return df[df[attribute] == value].reset_index(drop = True) #reset_index(drop=True) được sử dụng để thiết lập lại chỉ số của các dòng trong phụ-bảng, bỏ qua chỉ số ban đầu và tạo một chỉ số mới bắt đầu từ 0.\n","    #xây dựng cây\n","    def buildtree(self,df, tree = None):\n","      self.dt_Train=df\n","      self.Class = df.keys()[-1]\n","      node = self.find_winner(df)\n","      attvalue = np.unique(df[node])\n","      if tree is None:\n","        tree = {} #tạo từ điển rỗng\n","        tree[node] = {}\n","      for value in attvalue:\n","        subtable = self.get_subtable(df,node,value)\n","        Clvalue, counts = np.unique(subtable[self.Class], return_counts = True)  #Clvalue: mảng chứa giá trị duy nhất của cột nhãn, counts: mảng chứa số lần xuất hiện của từng nhãn trong subtable\n","        if len(counts) == 1:\n","          tree[node][value] = Clvalue[0]\n","        else:\n","          tree[node][value] = self.buildtree(subtable)\n","      self.tree=tree\n","      return tree\n","    #dự đoán nhãn\n","    def predict_one(self,inst):\n","      tree = self.buildtree(self.dt_Train)\n","      for node in tree.keys():  #tên node root\n","        value = inst[node]\n","        if node in tree and value in tree[node]:  tree = tree[node][value]\n","        else: tree = random.choice(dt_Test[self.Class].unique())\n","        prediction = 0\n","        if type(tree) is dict:\n","          prediction = self.predict(inst, tree)\n","        else:\n","          prediction = tree\n","      return prediction\n","    def predict(self,dt_Test):\n","      for i in range(len(dt_Test)):\n","        inst = dt_Test.iloc[i,:]\n","        prediction = self.predict_one(inst)\n","        Y_label.append(prediction)\n","      return Y_label\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/work.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.3 , shuffle = False)\n","\n","# #tính entropy: I\n","# def find_entropy(df):\n","#   Class = df.keys()[-1]\n","#   values = df[Class].unique()\n","#   entropy = 0\n","#   for value in values:\n","#     prob = df[Class].value_counts()[value]/len(df[Class])\n","#     entropy += -prob * np.log2(prob)\n","#   return float(entropy)\n","\n","\n","# # tính entropy attribute: E\n","# def find_entropy_attribute(df, attribute):\n","#   Class = df.keys()[-1]\n","#   target_values = df[Class].unique()\n","#   attribute_values = df[attribute].unique()\n","#   avg_entropy = 0\n","#   for value in attribute_values:\n","#     entropy = 0\n","#     for value1 in target_values:\n","#       num = len(df[attribute][df[attribute] == value][df[Class] == value1]) #len(df[(df[attribute] == value) & (df[Class] == value1)])\n","#       den = len(df[attribute][df[attribute] == value])\n","#       prob = num/den\n","#       entropy += -prob * np.log2(prob + 0.000001)\n","#     avg_entropy += (den/len(df))*entropy\n","#   return float(avg_entropy)\n","# #xây dựng cây\n","# def buildtree(self,df, tree = None):\n","#   node = find_winner(df)\n","#   attvalue = np.unique(df[node])\n","#   if tree is None:\n","#     tree = {} #tạo từ điển rỗng\n","#     tree[node] = {}\n","#   for value in attvalue:\n","#     subtable = get_subtable(df,node,value)\n","#     Clvalue, counts = np.unique(subtable[self.Class], return_counts = True)  #Clvalue: mảng chứa giá trị duy nhất của cột nhãn, counts: mảng chứa số lần xuất hiện của từng nhãn trong subtable\n","#     if len(counts) == 1:\n","#       tree[node][value] = Clvalue[0]\n","#     else:\n","#       tree[node][value] = buildtree(subtable)\n","#   return tree\n","\n","# #Find Winner: Gain\n","# def find_winner(df):\n","#   IG = []\n","#   for key in df.keys()[:-1]:\n","#     IG.append(find_entropy(df) - find_entropy_attribute(df, key))\n","#   return df.keys()[:-1][np.argmax(IG)]\n","#   #np.argmax(IG): trả về chỉ mục có Gain lớn nhất\n","#   #df.keys()[:-1][np.argmax(IG)] truy cập vào danh sách tên cột, loại bỏ cột cuối cùng và trả về tên cột có chỉ mục tương ứng với Information Gain (IG) lớn nhất.\n","\n","\n","# #lấy bảng phụ\n","# def get_subtable(df, attribute, value):\n","#   return df[df[attribute] == value].reset_index(drop = True) #reset_index(drop=True) được sử dụng để thiết lập lại chỉ số của các dòng trong phụ-bảng, bỏ qua chỉ số ban đầu và tạo một chỉ số mới bắt đầu từ 0.\n","\n","\n","# #xây dựng cây\n","# def buildtree(df, tree = None):\n","#   node = find_winner(df)\n","#   attvalue = np.unique(df[node])\n","#   Class = df.keys()[-1]\n","#   if tree is None:\n","#     tree = {} #tạo từ điển rỗng\n","#     tree[node] = {}\n","#   for value in attvalue:\n","#     subtable = get_subtable(df,node,value)\n","#     Clvalue, counts = np.unique(subtable[Class], return_counts = True)  #Clvalue: mảng chứa giá trị duy nhất của cột nhãn, counts: mảng chứa số lần xuất hiện của từng nhãn trong subtable\n","#     if len(counts) == 1:\n","#       tree[node][value] = Clvalue[0]\n","#     else:\n","#       tree[node][value] = buildtree(subtable)\n","#   return tree\n","\n","# tree = buildtree(dt_Train)\n","# print(\"- CÂY QUYẾT ĐỊNH: \")\n","# pprint.pprint(tree)\n","\n","\n","#dự đoán nhãn\n","# def predict(inst, tree):\n","#   for node in tree.keys():  #tên node root\n","#     value = inst[node]\n","#     if node in tree and value in tree[node]:  tree = tree[node][value]\n","#     else: tree = random.choice(dt_Test[\"quit job\"].unique())\n","#     prediction = 0\n","#     if type(tree) is dict:\n","#       prediction = predict(inst, tree)\n","#     else:\n","#       prediction = tree\n","#   return prediction\n","\n","\n","Y_label = []\n","for i in range(len(dt_Test)):\n","  inst = dt_Test.iloc[i,:]\n","  prediction = predict(inst, tree)\n","  Y_label.append(prediction)\n","\n","print(\"\\n- NHÃN DỰ ĐOÁN TRÊN TẬP TEST:\\n\",Y_label)\n","\n","#dự đoán nhãn trên 1 mẫu dữ liệu:\n","mau1 = {\"work environment\": \"ordinary\", \"experience\": \"med\", \"passion\": \"small\", \"advancement speed\": \"normal\",\"salary\":\"med\"}\n","print(\"- Nhãn dự đoán của mẫu 1: \",predict(mau1, tree))\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/winedata.csv')\n","dt_Train, dt_Test = train_test_split(df, test_size=0.2 , shuffle = True)\n","\n","le=preprocessing.LabelEncoder()\n","df=df.apply(le.fit_transform)\n","\n","dt_Train, dt_Test = train_test_split(df, test_size=0.2 , shuffle = True)\n","\n","X_train = dt_Train.drop(['Class'],axis=1)\n","y_train = dt_Train['Class']\n","X_test = dt_Test.drop(['Class'],axis=1)\n","y_test = dt_Test['Class']\n","\n","pla = DecisionTreeClassifier(criterion='entropy')\n","pla.fit(X_train, y_train)\n","y_predict = pla.predict(X_test)\n","y_predict=np.array(y_predict)\n","y_test=np.array(y_test)\n","\n","print(\"Accuracy: \", accuracy_score(y_test, y_predict))\n","print(\"Precision: \", precision_score(y_test, y_predict,average=\"macro\"))\n","print(\"Recall: \", recall_score(y_test, y_predict,average=\"macro\"))\n","print(\"F1: \", f1_score(y_test, y_predict,average=\"macro\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"sgBdbzhHQxM-","executionInfo":{"status":"error","timestamp":1702712408065,"user_tz":-420,"elapsed":1572,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"cceb02a9-9aca-48a4-827b-b29e1d97857f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9f2986af7bbc>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mY_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mY_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'work.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mdt_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'work.csv'"]}]},{"cell_type":"code","source":["from math import sqrt\n","while True:\n","\n","  a=float(input())\n","  b=float(input())\n","  x=float(input())\n","  y=float(input())\n","  dist=sqrt((a-x)**2+(b-y)**2)\n","  print(dist)\n"],"metadata":{"id":"tI9-Aip6NjmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","import pandas as pd\n","\n","\n","class Node:\n","    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n","        self.feature = feature\n","        # Nguong de chia 1 thuoc tinh thanh 2 phan\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        #value la yes or no (nhan~)\n","        self.value = value\n","\n","    # neu gia tri ton tai thi do la nut la\n","    def is_leaf_node(self):\n","        return self.value is not None\n","\n","class DecisionTree:\n","    def __init__(self, min_samples_split=20, max_depth=50, n_features=None):\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        #neu khong chon ngau nhien thi tat cac cac feature duoc xem xet\n","        self.n_features = n_features\n","        self.root = None\n","    def fit(self, X, y):\n","        #shape[1] tra ve so cot cua bang\n","        # neu n_features la None thi dung toan bo features\n","        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n","        self.root = self._grow_tree(X, y)\n","\n","    def _grow_tree(self, X, y, depth=0):\n","        #lay ra so hang va so cot cua du lieu\n","        n_samples, n_feats = X.shape\n","        # xem co bao nhieu nhan~\n","        n_labels = len(np.unique(y))\n","        # dieu kien dung, chi co 1 nhan~ --> nut la, qua sau, qua it mau~\n","        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n","\n","            #tim nhan pho bien nhat\n","            leaf_value = self._most_common_label(y)\n","            return Node(value=leaf_value)\n","\n","        # chon ra cac thuoc tinh con lai moi lan xay nut moi... roi tim thuoc tinh tot nhat trong cac thuoc tinh do....\n","        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n","\n","        #find the best split\n","        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n","\n","        # create child nodes\n","        # left_idxs chua cac chi muc cua cac hang <= best_thresh\n","        # right_idxs chua cac chi muc cua cac hang > best_thresh\n","        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n","        #X[left_idxs, :] la lay tat cac cac hang co chi muc trong left_idxs\n","        #y[left_idxs] la lay tat cac cac nhan~ ung' voi so hang cua left_idxs\n","        # xay cay dua tren cac X va y moi'\n","        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n","        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n","\n","        return Node(best_feature, best_thresh, left, right)\n","    def _best_split(self, X, y, feat_idxs):\n","        best_gain = -1\n","        split_idx, split_threshold = None, None\n","\n","        for feat_idx in feat_idxs:\n","            #lay ra tat ca cac gia tri cua cot ung voi index\n","            X_column = X[:, feat_idx]\n","            # tu cac gia tri cua cot lay ra cac gia tri unique cua cot, de tim xem gia tri nao chia cot thanh 2 phan ma tinh ig to nh....\n","            thresholds = np.unique(X_column)\n","            #voi moi~ thuoc tinh , tim gia tri chia cot do thanh 2 phan ma cho ra ig cao nhat roi so sanh voi best_gain\n","            for thr in thresholds:\n","                #information gain\n","                gain = self._information_gain(y, X_column, thr)\n","\n","                if (gain > best_gain):\n","                    best_gain = gain\n","                    split_idx = feat_idx\n","                    split_threshold = thr\n","\n","        return split_idx, split_threshold\n","\n","    def _information_gain(self, y, X_column, threshold):\n","        #tinh entropy cua cha (const hang so)\n","        parent_entropy = self._entropy(y)\n","\n","        # tao con, chia thuoc tinh thanh 2 nhanh theo nguong~ --> vi du <= 25 va > 25\n","        left_idxs, right_idxs = self._split(X_column, threshold)\n","\n","        #neu chia theo nguong~ ma tat ca cac thuoc tinh ben trai hoac ben phai (deu lon hon 25 )\n","        if len(left_idxs) == 0 or len(right_idxs) == 0:\n","            return 0\n","\n","        #tinh avg entropy cua con (cong thuc)\n","        n = len(y)\n","        n_l, n_r = len(left_idxs), len(right_idxs)\n","        # chua hieu, left_idxs = [2, 3, 4] thi y[left_idxs] la cac nhan~ ung voi gia tri 2, 3, 4\n","        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n","        # n se thay doi qua tung nut\n","        # cong thuc tinh entropy cua 1 feature\n","        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n","        #tinh ig\n","        information_gain = parent_entropy - child_entropy\n","\n","        return information_gain\n","\n","    def _split(self, X_column, split_thresh):\n","        # tra ve mang cac gia tri <= nguong va > nguong\n","        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n","        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n","\n","        return left_idxs, right_idxs\n","\n","    #cong thuc entropy: -xichma p(X) * log2(p(X))  --> p(x) = yes / n, no / n\n","    def _entropy(self, y):\n","        #[1, 2, 3, 1, 2] --> [0, 2, 2, 1] --> 0 xuat hien 0 lan, 1 xuat hien 2, 2 xuat hien 2.....\n","        hist = np.bincount(y)\n","        # lay tung phan tu trong mang chia cho len(y), se dc 1 mang cac p(x)\n","        ps = hist / len(y)\n","\n","        return -np.sum([p * np.log(p) for p in ps if p > 0])\n","\n","    def _most_common_label(self, y):\n","        #chuyen thanh map voi key la phantu, value la so lan xuat h\n","        counter = Counter(y)\n","        #(1) la tra ve tuple chua cap key, value xuat hien nhieu nhat, 00 la tro den k\n","        value = counter.most_common(1)[0][0]\n","\n","        return value\n","\n","    def predict(self, X):\n","        return np.array([self._traverse_tree(x, self.root) for x in X])\n","\n","    def _traverse_tree(self, x, node):\n","        if node.is_leaf_node():\n","            return node.value\n","\n","        if x[node.feature] <= node.threshold:\n","            return self._traverse_tree(x, node.left)\n","        return self._traverse_tree(x, node.right)\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/new_train.csv\")\n","data['y'] = data['y'].map({'no': 0, 'yes': 1})\n","\n","dt_Train, dt_Test = train_test_split(data, test_size=0.3, random_state=1234)\n","\n","X_train = dt_Train.iloc[:, :15]\n","y_train = dt_Train.iloc[:, 15]\n","X_test = dt_Test.iloc[:, :15]\n","y_test = dt_Test.iloc[:, 15]\n","\n","\n","\n","label=['job',\t'marital',\t'education',\t'default',\t'housing',\t'loan',\t'contact',\t'month',\t'day_of_week'\t,'poutcome']\n","\n","encoders = {}\n","for i in label:\n","    encoders[i]=LabelEncoder()\n","    X_train[i] = encoders[i].fit_transform(X_train[i])\n","for i in label:\n","    X_test[i] = encoders[i].transform(X_test[i])\n","\n","X_train=np.array(X_train)\n","y_train=np.array(y_train)\n","X_test=np.array(X_test)\n","y_test=np.array(y_test)\n","clf = DecisionTree(max_depth=13)\n","clf.fit(X_train, y_train)\n","\n","\n","y_predict = clf.predict(X_test)\n","y_test = np.array(y_test)\n","y_predict = np.array(y_predict)\n","\n","\n","print(\"Accuracy: \", accuracy_score(y_test, y_predict))\n","print(\"Precision: \", precision_score(y_test, y_predict, average=\"macro\", zero_division=1))\n","print(\"Recall: \", recall_score(y_test, y_predict, average=\"macro\"))\n","print(\"F1: \", f1_score(y_test, y_predict, average=\"macro\"))\n"],"metadata":{"id":"G-vre2LfumTh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703664842388,"user_tz":-420,"elapsed":11426,"user":{"displayName":"Khang Nguyễn","userId":"17174970936019069150"}},"outputId":"efd86d86-5f97-4dc5-ec5c-c275a9f52d05"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.8990389479008599\n","Precision:  0.7530054701822139\n","Recall:  0.7009931113952583\n","F1:  0.7226650544412813\n"]}]}]}